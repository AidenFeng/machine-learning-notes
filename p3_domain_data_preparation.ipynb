{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "p3_domain_data_preparation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xopl4yvRm5Bb",
        "colab_type": "text"
      },
      "source": [
        "# Project 3: Domain Data Preparation\n",
        "This project will help you familiarize with common data collection and preprocessing tasks. We will mine public text data related to the topic of coronavirus from different sources, and process and combine them to a single corpus, on which we can perform feature engineering to prepare for subsequent analyses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "excluded_from_script"
        ],
        "id": "f4xhNbhRm5Bd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f297812-1ecf-4a22-a351-46bf1ddc8c33"
      },
      "source": [
        "# Run this cell to complete all the environment setups on Colab\n",
        "# You do not need to upload anything else to your Colab environment after running this cell\n",
        "# This cell has been tagged with excluded_from_script, it will be ignored by the autograder\n",
        "\n",
        "# download and extract handout to current dir\n",
        "!rm -rf p3_handout\n",
        "!mkdir p3_handout\n",
        "!wget -qc http://clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-domain-data-preparation/colab/p3_handout_v4.tgz -O - | tar -xz -C p3_handout\n",
        "!rm -f p3_handout/*.ipynb\n",
        "!mv p3_handout/* .\n",
        "\n",
        "# install relevant packages\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# install chromedriver\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver ."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.18.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/27/e35e7c6e6a52fab9fcc64fc2b20c6b516eba930bb02b10ace3b38200d3ab/numpy-1.18.4-cp36-cp36m-manylinux1_x86_64.whl (20.2MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20.2MB 1.3MB/s \n",
            "\u001b[?25hCollecting pandas==1.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/71/8f53bdbcbc67c912b888b40def255767e475402e9df64050019149b1a943/pandas-1.0.3-cp36-cp36m-manylinux1_x86_64.whl (10.0MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10.0MB 37.1MB/s \n",
            "\u001b[?25hCollecting nltk==3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.4MB 38.4MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/25/ff030e2437265616a1e9b25ccc864e0371a0bc3adb7c5a404fd661c6f4f6/beautifulsoup4-4.9.1-py3-none-any.whl (115kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 122kB 42.2MB/s \n",
            "\u001b[?25hCollecting selenium==3.141.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 911kB 43.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (2.23.0)\n",
            "Collecting pdfminer.six==20200517\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/c0/ef1c8758bbd86edb10b5443700aac97d0ba27a9ca2e7696db8cd1fdbd5a8/pdfminer.six-20200517-py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.6MB 22.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.3->-r requirements.txt (line 2)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.3->-r requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk==3.5->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk==3.5->-r requirements.txt (line 3)) (0.15.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk==3.5->-r requirements.txt (line 3)) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk==3.5->-r requirements.txt (line 3)) (4.41.1)\n",
            "Collecting soupsieve>1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/8f/457f4a5390eeae1cc3aeab89deb7724c965be841ffca6cfca9197482e470/soupsieve-2.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium==3.141.0->-r requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.23.0->-r requirements.txt (line 6)) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.23.0->-r requirements.txt (line 6)) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.23.0->-r requirements.txt (line 6)) (3.0.4)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six==20200517->-r requirements.txt (line 7)) (2.2.2)\n",
            "Collecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/55/17fa0b55849dc135f7bc400993a9206bf06d1b5d9520b0bc8d47c57aaef5/pycryptodome-3.9.8-cp36-cp36m-manylinux1_x86_64.whl (13.7MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13.7MB 31.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 8)) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 8)) (5.2.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 8)) (4.10.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 8)) (4.7.5)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 8)) (5.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 8)) (7.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas==1.0.3->-r requirements.txt (line 2)) (1.12.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 8)) (5.3.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 8)) (5.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 8)) (2.1.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 8)) (1.0.18)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (4.5.3)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (0.8.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (2.11.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (4.6.3)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (4.3.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (5.0.7)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 8)) (1.9.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 8)) (19.0.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (0.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (3.1.5)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (1.4.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (0.4.4)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 8)) (3.5.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 8)) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 8)) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 8)) (47.3.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 8)) (4.4.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 8)) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 8)) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (0.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (1.1.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (2.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (20.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (0.5.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (2.4.7)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434673 sha256=05190dc6f8e396446bfb867ef38d3ee203d6dd56d2bda8c481f2023dd36d769a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, pandas, nltk, soupsieve, beautifulsoup4, selenium, pycryptodome, pdfminer.six\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: pandas 1.0.5\n",
            "    Uninstalling pandas-1.0.5:\n",
            "      Successfully uninstalled pandas-1.0.5\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed beautifulsoup4-4.9.1 nltk-3.5 numpy-1.18.4 pandas-1.0.3 pdfminer.six-20200517 pycryptodome-3.9.8 selenium-3.141.0 soupsieve-2.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Hit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
            "Get:11 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [93.7 kB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [41.2 kB]\n",
            "Get:16 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,845 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [89.0 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [868 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [102 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,301 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [9,282 B]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,000 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,405 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [13.6 kB]\n",
            "Get:25 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [890 kB]\n",
            "Fetched 7,932 kB in 4s (2,118 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 55 not upgraded.\n",
            "Need to get 75.5 MB of archives.\n",
            "After this operation, 256 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 83.0.4103.61-0ubuntu0.18.04.1 [1,119 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 83.0.4103.61-0ubuntu0.18.04.1 [66.7 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 83.0.4103.61-0ubuntu0.18.04.1 [3,378 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 83.0.4103.61-0ubuntu0.18.04.1 [4,294 kB]\n",
            "Fetched 75.5 MB in 7s (10.9 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 144379 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_83.0.4103.61-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (83.0.4103.61-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_83.0.4103.61-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (83.0.4103.61-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_83.0.4103.61-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (83.0.4103.61-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_83.0.4103.61-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (83.0.4103.61-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (83.0.4103.61-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (83.0.4103.61-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (83.0.4103.61-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (83.0.4103.61-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx2sHYfIm5Bh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import json\n",
        "import collections\n",
        "import time\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.common import exceptions\n",
        "\n",
        "from pdfminer import high_level"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "excluded_from_script"
        ],
        "id": "rzQbeKQ8m5Bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this cell has been tagged with excluded_from_script\n",
        "# it will not be run by the autograder\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppo4aqqZm5Bn",
        "colab_type": "text"
      },
      "source": [
        "## Part A: Text Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqQwuC_Jm5Bn",
        "colab_type": "text"
      },
      "source": [
        "Text data on the internet is very messy.  Typically there is a fair amount of processing work to do once you have collected any sizeable chunk of text data, in order to have it ready for subsequent analyses. To get you familiar with this kind of data, this section will walk you through some common processing tasks:\n",
        "\n",
        "The first step is to import the lemmatizer and set of English stopwords from `nltk`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPmnv6qvm5Bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download(\"stopwords\", quiet = True)\n",
        "nltk.download(\"wordnet\", quiet = True)\n",
        "nltk.download(\"punkt\", quiet = True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet = True)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "english_stopwords = set(nltk.corpus.stopwords.words('english'))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1cshhyEm5Br",
        "colab_type": "text"
      },
      "source": [
        "### Question 1: Text cleaning and tokenization\n",
        "Now implement the function `preprocess_text` which, given a text string, returns a processed list of tokens, where each token:\n",
        "1. is in lower case\n",
        "1. appears in the same order as in the input string.\n",
        "1. is in its lemmatized form, if one exists. If a word cannot be lemmatized, do not include it in the output.\n",
        "1. does not contain any characters other than letters and digits:\n",
        "   * remove trailing `'s`: `Children's` becomes `children`\n",
        "   * omit other apostrophes: `don't` becomes `dont`\n",
        "   * break tokens at other punctuation and non-ascii characters: `word-of-mouth` becomes `[\"word\", \"of\", \"mouth\"]`.\n",
        "1. is not a stopword (after lemmatization)\n",
        "1. contains at least two characters (after lemmatization)\n",
        "\n",
        "**Notes**:\n",
        "* To detect punctuation and non-ascii characters, you can use `string.punctuation` and `string.ascii_letters` respectively.\n",
        "* When lemmatizing a word, you should also specify the part-of-speech `pos` parameter. This can be obtained by calling `nltk.pos_tag()` and using the first returned tag (in case there are multiple possibilities). You can interpret the returned tag as follows:\n",
        "    * If it starts with \"J\", it is an adjective.\n",
        "    * If it starts with \"V\", it is a verb.\n",
        "    * If it starts with \"R\", it is an adverb.\n",
        "    * Otherwise, it is a noun.\n",
        "* Different order of operations like removing punctuation, tokenization and lemmatization will yield different results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti9Fmiilm5Br",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(text, stopwords = {}):\n",
        "    \"\"\"\n",
        "    Process the input text and turn it into a list of cleaned, lemmatized words\n",
        "    \n",
        "    args:\n",
        "        text (str) : the input text\n",
        "    \n",
        "    kwargs:\n",
        "        stopwords (Set[str]) : the set of stopwords to exclude\n",
        "    \n",
        "    return:\n",
        "        List[str] : the list of tokenized words from the input text\n",
        "    \"\"\"\n",
        "\n",
        "    def pos_conversion(tokenized_text):\n",
        "      '''\n",
        "      Convert nltk.pos_tag result to lemmatizer.lemmatize.pos input\n",
        "      args:\n",
        "          tokenized_text (list of str) : e.g., ['Today', 'is', 'a', 'good', 'day']      \n",
        "      return:\n",
        "        str : str with each word been properly lemmatized with a pos\n",
        "      '''\n",
        "      # Call pos_tag on each word individually as required in this case and generate list of tuples\n",
        "      pos_tag_results = [nltk.pos_tag([token])[0] for token in tokenized_text]\n",
        "      # Transformation of pos tags\n",
        "      new_pos_tag_results = []\n",
        "      for element in pos_tag_results:\n",
        "        key, value = element\n",
        "        if value.startswith(\"J\"):\n",
        "          new_pos_tag_results.append((key, \"a\"))\n",
        "        elif value.startswith(\"V\"):\n",
        "          new_pos_tag_results.append((key, \"v\"))\n",
        "        elif value.startswith(\"R\"):\n",
        "          new_pos_tag_results.append((key, \"r\"))\n",
        "        else:\n",
        "          new_pos_tag_results.append((key, \"n\"))\n",
        "      # Lemmatize based on individual tags\n",
        "      return \" \".join(lemmatizer.lemmatize(element[0], pos = element[1]) for element in new_pos_tag_results)\n",
        "\n",
        "    # Lowercase the string\n",
        "    text = text.lower()\n",
        "    # Remove trailing 's\n",
        "    text = re.sub(\"\\'s\", \"\", text)\n",
        "    # Omit other apostrophes\n",
        "    text = re.sub(\"\\'\", \"\", text)\n",
        "    # break tokens at other punctuation\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), \" \", text)\n",
        "    # break tokens at non-ascii characters\n",
        "    text = re.sub('[^%s]' % re.escape(string.ascii_letters + '0123456789'), \" \", text)\n",
        "    # Lemmatization (the order about where to put this is very important!)\n",
        "    text = pos_conversion(nltk.word_tokenize(text))\n",
        "    # Remove stopwords\n",
        "    text = \" \".join([word for word in text.split(\" \") if word not in stopwords])\n",
        "    # Each token contains at least two characters\n",
        "    text = \" \".join([word for word in text.split(\" \") if len(word) > 1])\n",
        "    return text.split(\" \") if text.split(\" \") != [''] else []"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "excluded_from_script"
        ],
        "id": "YH9HiCWAm5Bv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ddf35956-ca6a-4e8e-bb43-adbf6d5cb58b"
      },
      "source": [
        "def test_preprocess_text():\n",
        "    # lowercase\n",
        "    assert preprocess_text(\"I like Data Science\") == ['like', 'data', 'science']\n",
        "    \n",
        "    # tokenization\n",
        "    assert preprocess_text(\" ab..ab. .ab . ab.\")== ['ab', 'ab', 'ab', 'ab']\n",
        "    assert preprocess_text(\"word-of-mouth hello-world\")== ['word', 'of', 'mouth', 'hello', 'world']\n",
        "    assert preprocess_text(\"you've\")== ['youve']\n",
        "    assert preprocess_text(\"gotta\")== ['get', 'ta']\n",
        "    assert preprocess_text(\"hello_world\") == [\"hello\", \"world\"]\n",
        "    \n",
        "    # apostrophe handling\n",
        "    assert preprocess_text(\"She's\")== ['she']\n",
        "    assert preprocess_text(\"car, cars, car's, cars'\")== ['car', 'car', 'car', 'car']\n",
        "    \n",
        "    # lemmatizer\n",
        "    assert preprocess_text(\"cats\") == ['cat']\n",
        "    assert preprocess_text(\"DID\") == ['do']\n",
        "    \n",
        "    # part-of-speech\n",
        "    assert preprocess_text(\"CMU is Cool\") == ['cmu', 'be', 'cool']\n",
        "    \n",
        "    # unicode handling\n",
        "    assert preprocess_text(\"hope thisüëèwill work\") == ['hope', 'this', 'will', 'work']\n",
        "    \n",
        "    # stopwords\n",
        "    assert preprocess_text(\"the weather is really nice\", english_stopwords) == ['weather', 'really', 'nice']\n",
        "    assert preprocess_text(\n",
        "        \"To apply SVM learning in partial discharge classification, data input is very important!?\",\n",
        "        english_stopwords\n",
        "    ) =='apply svm learn partial discharge classification data input important'.split()\n",
        "    assert preprocess_text(\"after all he's done\", english_stopwords) == []\n",
        "    assert preprocess_text(\"they didn‚Äôt have much chance of guessing what it was without further clues.\", english_stopwords) == ['much', 'chance', 'guess', 'without', 'far', 'clue']\n",
        "    \n",
        "    # additional from piazza\n",
        "    assert preprocess_text(\"hello_world\") == [\"hello\", \"world\"]\n",
        "    assert preprocess_text(\"DUQUE'S\", english_stopwords) == [\"duque\"]\n",
        "    assert preprocess_text( \"'shed'\") == ['hed']\n",
        "    assert preprocess_text(\"'fake news'\") == ['fake', 'news']\n",
        "    assert preprocess_text(\"the 'rona\", english_stopwords) == ['rona']\n",
        "    # test on long text string\n",
        "    with open(\"local_test_refs/henrys_letter.txt\") as infile, open(\"local_test_refs/processed_henrys_letter.txt\") as outfile:\n",
        "        processed_str = preprocess_text(infile.read())\n",
        "        reference_str = outfile.read().splitlines()\n",
        "        assert processed_str == reference_str\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_preprocess_text()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLv3QCZjm5Bz",
        "colab_type": "text"
      },
      "source": [
        "You may notice that the lemmatization functionality isn't perfect; for example, it would map `\"as\"` to `\"a\"` because `\"as\"` is being treated as a noun instead of a proposition (with tag `\"IN\"`). In general, identifying the correct part-of-speech tag is very context-dependent (for example, \"clear\" can be either an adjvective, adverb, verb or noun). In the context of this project, we will not dive deep into these linguistic nuances, and settle with the lemmatization rules above.\n",
        "\n",
        "The above processing function already covers a fair number of text cleaning tasks. We can now begin to collect data from online sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nyuLXkTm5Bz",
        "colab_type": "text"
      },
      "source": [
        "## Part B: Tweet Mining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpY16_4tm5B0",
        "colab_type": "text"
      },
      "source": [
        "Twitter is among the most popular social media; according to [Omnicore](https://www.omnicoreagency.com/twitter-statistics/#:~:text=There%20are%2048.35%20million%20monthly,monetizable%20daily%20active%20Twitter%20users.), it has 48.35 million active users, 42% of whom use Twitter on a daily basis. Furthermore, tweets are public by default, making the site a particularly rich data source on any given trending topic.\n",
        "\n",
        "In this section, you will extract tweets related to the topic of coronavirus. It should be noted that, due to the dynamic nature of Twitter, any user can edit or remove their old tweets, making it difficult to obtain deterministic results (and to autograde your code). Therefore, we will instead use a fixed Tweet [dataset from Kaggle](https://www.kaggle.com/smid80/coronavirus-covid19-tweets) and provide you with a custom API to query tweets from this dataset. For your own data science projects in the future, however, you are encouraged to explore the [official Twitter API](https://developer.twitter.com/en/docs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68QbSpGOm5B0",
        "colab_type": "text"
      },
      "source": [
        "### Question 2: Retrieve starting tweets\n",
        "Implement the function `get_tweets` that sends a GET request to https://gettweets.azurewebsites.net/11637/tweets and returns the status code as well as the response JSON. The response JSON is a list of dictionaries, each corresponding to one tweet and having the following format:\n",
        "\n",
        "```python\n",
        "{\n",
        "    'text': 'hello world', # str, the tweet content\n",
        "    'lang': 'en', # str, the tweet language\n",
        "    'id': 123, # int, the tweet id\n",
        "    'time': '2019-12-04' # # str, yyyy-mm-dd\n",
        "}\n",
        "```\n",
        "\n",
        "**Notes**:\n",
        "* The API endpoint url https://gettweets.azurewebsites.net/11637/tweets is provided in the global variable `TWEET_API`.\n",
        "* You should call `.text` on the response object to retrieve its content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4ZTpjgrm5B1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TWEET_API = \"https://gettweets.azurewebsites.net/11637/tweets\"\n",
        "\n",
        "def get_tweets(url = TWEET_API):\n",
        "    \"\"\"\n",
        "    Retrieve tweets by sending a GET request to the provided API endpoint.\n",
        "    \n",
        "    params:\n",
        "        url (str) : the url to send request to\n",
        "    \n",
        "    return:\n",
        "        Tuple(status_code, response):\n",
        "            status_code (int) : the status code of the request\n",
        "            response (List[Dict[str, str]]) : the JSON response, which is a list of dictionaries,\n",
        "                one for each tweet\n",
        "    \"\"\"\n",
        "    response = requests.get(TWEET_API)\n",
        "    return response.status_code, response.text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "excluded_from_script"
        ],
        "id": "07Cp_U33m5B4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "136e47bb-1b68-4a53-8279-2457ba393c68"
      },
      "source": [
        "def test_get_tweets():\n",
        "    response_code, tweets = get_tweets()\n",
        "    assert response_code == 200\n",
        "    tweet_jsons = json.loads(tweets)\n",
        "    assert len(tweet_jsons) == 100\n",
        "    first10_tweet_ids = [tweet_json[\"id\"] for tweet_json in tweet_jsons[:10]]\n",
        "    assert first10_tweet_ids == [2819, 3075, 3331, 3587, 3843, 4099, 4355, 4611, 4867, 5123]\n",
        "    assert tweet_jsons[1][\"text\"] == 'Los Angeles County has identified 6 new cases of the coronavirus &amp; declared a local state of emergency.\\n#CoronavirusOutbreak #Coronavirusflorida #LosAngeles'\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_get_tweets()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-m7cxJ3m5B7",
        "colab_type": "text"
      },
      "source": [
        "Great, you now have 100 tweets at your disposal! Typically though, we would like to have more flexibility in our search, for example by specifying particular parameters that indicate our search objective. In this case, our API provides four parameters as follows:\n",
        "\n",
        "* `lang` - specify the tweet language, which is an [ISO 639-1 code](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes), e.g., `en` for English.\n",
        "* `start` - the start date, formatted as yyyy-mm-dd; only tweets created on or after this date are returned.\n",
        "* `end` - the end date, formatted as yyyy-mm-dd; only tweets created on or before this date are returned.\n",
        "* `page` - the tweet page number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73xdeQbcm5B8",
        "colab_type": "text"
      },
      "source": [
        "Note the use of the `page` parameter here. Because returning a large JSON would put burden on the server, we have restricted all response JSONs to only include 100 tweets. To get more tweets that also satisfy your search query, you can specify the `page` parameter based on the following formula: if `page = i` (indexed from 1) then the tweets from index `100*(i-1) + 1` to index `100*i` (inclusive) will be returned.\n",
        "\n",
        "As an example, if your configuration of `(lang, start, end)` yields 306 tweets, then:\n",
        "\n",
        "* `page = 1` or no `page` specified will return the tweets 1-100.\n",
        "* `page = 2` will return the tweets 101-200.\n",
        "* `page = 3` will return the tweets 201-300.\n",
        "* `page = 4` will return the tweets 301-306.\n",
        "* `page = 5` or larger will return a JSON with empty content `\"[]\"`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqHZtN3Gm5B8",
        "colab_type": "text"
      },
      "source": [
        "### Question 3: Search for tweets with parameters\n",
        "Implement the function `get_tweet_texts_with_params` that sends a GET request to the provided API endpoint with (optional) input parameters `lang`, `start`, `end` and `page`. This function should collect all tweets that satisfy the search query if the number of such tweets is smaller than 10000, or the first 10000 tweets otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtB4KlH5m5B9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tweet_texts_with_params(url = TWEET_API, lang = \"all\", start = \"na\", end = \"na\", n_tweets = 10000):\n",
        "    \"\"\"\n",
        "    Search for tweets with parameters and extract their text content\n",
        "    \n",
        "    kwargs:\n",
        "        url (str) : the url to send request to\n",
        "        lang (str) : the tweet language in ISO 639-1 format\n",
        "        start (str) : the start date, yyyy-mm-dd\n",
        "        end (str) : the end date, yyyy-mm-dd\n",
        "        n_tweets (int) : the number of tweets to collect\n",
        "    \n",
        "    return:\n",
        "        List[str] : a list with the contents of the first n_tweets tweets returned from the search\n",
        "    \"\"\"\n",
        "    tweet_jsons = []\n",
        "    for i in range(int(n_tweets/100)):\n",
        "      response = requests.get(TWEET_API, params={'page': i+1, 'lang': lang, 'start': start, 'end': end})\n",
        "      tweet_jsons += json.loads(response.text)\n",
        "    return [x['text'] for x in tweet_jsons]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "tags": [
          "excluded_from_script"
        ],
        "id": "Jtp0Ss2am5CA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61c73df4-83f7-4ec5-fab7-fd5af493c0ab"
      },
      "source": [
        "def test_get_tweet_texts_with_params():\n",
        "    tweet_texts = get_tweet_texts_with_params(lang = \"en\", start = \"2020-03-09\", end = \"2020-03-09\")\n",
        "    assert len(tweet_texts) == 10000\n",
        "    example_tweets = [\n",
        "        '#coronavirus #COVID19 2 new cases reported in Kentucky (Harrison Co and Fayette Co), bring the states total to 6.',\n",
        "        \"#BREAKING\\n#Italy is extending its #coronavirus #quarantine measures, which include a ban on public gatherings, to the ENTIRE COUNTRY.\\nItaly's coronavirus death toll jumped on Monday by 97 to 463. \\nIt is the worst-hit country after #China.\\nhttps://t.co/Q6MNtyXttA\\n#COVID19\",\n",
        "        '#Investors√¢‚Ç¨‚Ñ¢ fortunes plunge as sell pressure hit banking stocks amid COVID-19 fear https://t.co/stihfvM7xZ via @MarketForcesA\\n\\n@Afrinvest @nsenigeria #equitymarket #fmcg #banks #insurance #oilandgas #COVID19',\n",
        "        'FROM A DOCTOR IN ITALY...\\n#coronavirus\\n#Coronavirusflorida\\n#CoronaVirusUpdates\\n#CoronavirusUSA\\n#COVID19 \\n#covid19Canada\\n#COVID19Toronto\\n#CoronaVirusCanada \\n#coronavirusToronto https://t.co/ZelZHWTuR1',\n",
        "        'Ina joint statement, Major League Baseball, Major League Soccer, the National Basketball Association, and the National Hockey League announced they are limiting locker room access due to concerns about the Coronavirus pandemic. #MLB #MLS #NBA #NHL #COVID19 #coronavirus https://t.co/Z6CUqxDrAO',\n",
        "        'Coronaviruses (CoV) are a large family of viruses that cause illness ranging from the common cold to more severe diseases such asMERS-CoVand SARS-CoV. A novel coronavirus (nCoV) is a new strain that has not been previously identified in humans.  #coronavirus #CoronavirusOutbreak',\n",
        "        'Simon Coveny and the entire Irish government attitude towards #covid19 just got destroyed on  #cblive',\n",
        "        'The reason for the toilet paper shortage is because when one person sneezes, 100 people shit themselves √∞≈∏¬§¬ß #coronavirus #COVID19 #tolietpaper',\n",
        "        'Latest #COVID19 numbers in Missouri - https://t.co/27vLQ3ZmqY',\n",
        "        'Tips on battling #CoronaVirusUpdate #COVID2019 #CoronavirusOutbreak \\nhttps://t.co/UaY8boPzHg'\n",
        "    ]\n",
        "    \n",
        "    assert tweet_texts[800:810] == example_tweets\n",
        "    emoji_tweet = \"Be carefull guy's and wish you all happy holi to you &amp; your family. :) \\n#HappyHoli #CoronavirusOutbreak #√†¬§¬π√†¬•‚Äπ√†¬§¬≤√†¬•‚Ç¨ #√†¬§¬π√†¬•‚Äπ√†¬§¬≤√†¬§¬ø√†¬§‚Ä¢√†¬§¬æ_√†¬§¬¶√†¬§¬π√†¬§¬® #BankLooteriBJP #Coronavid19 #marketcrash  #reliance #colours #KurkureWithSidNaaz #MondayMorning #MereAngneMein ##RangBarseWithSid #√†¬§¬¨√†¬•ÔøΩ√†¬§¬∞√†¬§¬æ_√†¬§¬®_√†¬§¬Æ√†¬§¬æ√†¬§¬®√†¬•‚Äπ_√†¬§¬π√†¬•‚Äπ√†¬§¬≤√†¬•‚Ç¨_√†¬§¬π√†¬•ÀÜ https://t.co/Rg2SpMNKZD\"\n",
        "    assert emoji_tweet in tweet_texts\n",
        "    print(\"All tests passed!\")\n",
        "    \n",
        "test_get_tweet_texts_with_params()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l7MQBFcm5CD",
        "colab_type": "text"
      },
      "source": [
        "### Question 4: Process tweet data\n",
        "Looking at some of the tweets above, we see that:\n",
        "1. Some tweets contain shortened URLs, for example `https://t.co/DzhsXPxUDa`. These are always in the form of `http://t.co/` or `https://t.co/` followed by 10 alphanumeric characters. These links should be removed.\n",
        "1. Some tweets contain emoticons such as `:)` or `<3`. The characters in these emoticons should be removed.\n",
        "\n",
        "Implement the function `process_tweet` that take as input a tweet text. This function will perform the above two cleaning steps and then call `preprocess_text` on the cleaned tweet.\n",
        "\n",
        "**Notes**:\n",
        "* We have provided a list of emoticons for you in the variable `emoticons`. You can assume that only elements in this set are considered emoticons and need to be removed.\n",
        "* Note that there may be no space between a shortened URL and the next word. However, you can assume that there are always 10 alphanumeric characters after http://t.co/ or https://t.co/.\n",
        "* Remember to specify the stopwords parameter `english_stopwords` when calling `preprocess_text`.\n",
        "* Side note: one of the inventors of emoticons is [Professor Scott Fahlman](https://www.cs.cmu.edu/~sef/sefSmiley.htm) in our own LTI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_VcxOa9m5CD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emoticons = [\n",
        "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', '<3',\n",
        "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "    ':-[', ':-<', '=\\\\', 'b=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "    ':c', ':{', '>:\\\\', ';('\n",
        "]\n",
        "\n",
        "\n",
        "def process_tweet(tweet_text):\n",
        "    \"\"\"\n",
        "    Process and tokenize tweets, in addition to removing URLs and emoticons\n",
        "    \n",
        "    args:\n",
        "        tweet_text (str) : a list of tweet contents\n",
        "    \n",
        "    return:\n",
        "        List[str] :  a list of processed tokens from the input tweet\n",
        "    \"\"\"\n",
        "    text = re.sub(\"(?:https|http)\\:\\/\\/t\\.co\\/(\\w){10}\", \" \", tweet_text)\n",
        "    for emot in emoticons:\n",
        "      text = text.replace(emot, \" \")\n",
        "    return preprocess_text(text, stopwords=english_stopwords)\n",
        "\n",
        "\n",
        "# do not modify this function\n",
        "def process_tweet_data(tweet_texts):\n",
        "    return [process_tweet(tweet_text) for tweet_text in tweet_texts]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "excluded_from_script"
        ],
        "id": "3KewCtR9m5CG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ad8d5b5-dced-49ad-bfa4-8bd86e411457"
      },
      "source": [
        "def test_process_tweet():\n",
        "    assert process_tweet(\"It's a great day :D\") == ['great', 'day']\n",
        "    assert process_tweet(\"<3hello\") == [\"hello\"]\n",
        "    assert process_tweet(\"goodX-Dday\") == [\"good\", \"day\"]\n",
        "    assert process_tweet(\"http://t.co/WJs5bmRthU,http://t.co/WJs5bmRthU,\") == []\n",
        "    assert process_tweet(\"hellohttp://t.co/WJs5bmRthUworld\") == [\"hello\", \"world\"]\n",
        "    assert process_tweet(\"http://taco/WJs5bmRthU\") == ['http', 'taco', 'wjs5bmrthu']\n",
        "    print(\"All tests passed!\")\n",
        "    \n",
        "test_process_tweet()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r4Z_HaDm5CI",
        "colab_type": "text"
      },
      "source": [
        "## Part C: Web Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJMdxXfym5CJ",
        "colab_type": "text"
      },
      "source": [
        "We now move to the second method of data extraction: using Selenium and Beautifulsoup to parse HTML codes. More speficically, we will collect news articles related to the same topic of Coronavirus from two major media outlets -- [Nature](https://www.nature.com/) and [The New York Times](https://www.nytimes.com/). Through this exercise, you will learn how to navigate HTML structures from different webpages in order to get the desired information.\n",
        "\n",
        "To begin, we have provided you with three helper functions:\n",
        "\n",
        "1. `retrieve_url` takes as input a webpage string URL and creates a BeautifulSoup object from the corresponding page content.\n",
        "1. `init_chromedriver` initializes the Selenium webdriver used with Chrome. If you specify `debug = True`, a new browser window will show up every time the function is called, allowing you to inspect the webpage and the effect of your Selenium code. Note that this only works if you are running Jupyter notebook locally; it does not work on Google Colab.\n",
        "1. `init_geckodriver` functions similarly and is used with Firefox.\n",
        "\n",
        "See the [Data Collection and Extraction Primer](https://nbviewer.jupyter.org/url/clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-data-collection-extraction-primer/data_collection_extraction_primer.ipynb) for detailed installation instructions. Our grader supports both the Chromium browser (version `83.0.4103.61-0ubuntu0.18.04.1`) and Firefox browser (version `77.0.1+build1-0ubuntu0.18.04.1`), so you can pick either `init` function to use in this assignment.\n",
        "\n",
        "**Notes**:\n",
        "* When you submit your code, remember to set `debug = False` in all of your function calls to `init_chromdriver` or `init_geckodriver`.\n",
        "* If you use chromedriver, edit the `PATH_TO_CHROMEDRIVER` variable to point to where the file is on your system. Similarly, if you use geckodriver, edit the variable `PATH_TO_GECKODRIVER`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmukkILCm5CJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def retrieve_url(url):\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "# path to the chromedriver executable, edit this if you use chromedriver locally\n",
        "# if you are on Colab, do not change this path\n",
        "# if you are on Windows, you may need to add .exe to the end of the path\n",
        "PATH_TO_CHROMEDRIVER = \"./chromedriver\"\n",
        "\n",
        "# path to the geckodriver executable, edit this if you use geckodriver locally\n",
        "# if you are on Windows, you may need to add .exe to the end of the path\n",
        "PATH_TO_GECKODRIVER = \"./geckodriver\"\n",
        "\n",
        "# do not modify this function\n",
        "def init_chromedriver(debug = False):\n",
        "    options = webdriver.ChromeOptions()\n",
        "    if not debug:\n",
        "        options.add_argument('--headless')\n",
        "        options.add_argument('--no-sandbox')\n",
        "        options.add_argument(\"--disable-setuid-sandbox\")\n",
        "        options.add_argument('--user-agent=\"\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36\"\"')\n",
        "    return webdriver.Chrome(executable_path = PATH_TO_CHROMEDRIVER, options = options)\n",
        "\n",
        "# do not modify this function\n",
        "def init_geckodriver(debug = False):\n",
        "    options = webdriver.FirefoxOptions()\n",
        "    if not debug:\n",
        "        options.add_argument('--headless')\n",
        "        options.add_argument('--no-sandbox')\n",
        "        options.add_argument(\"--disable-setuid-sandbox\")\n",
        "        options.add_argument('--user-agent=\"\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36\"\"')\n",
        "    return webdriver.Firefox(executable_path = PATH_TO_GECKODRIVER, options = options)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_qq4LHXm5CN",
        "colab_type": "text"
      },
      "source": [
        "### Question 5: Parsing a single article from Nature\n",
        "Implement the function `parse_page_nature` that takes as input a Nature news article string URL and return a JSON dictionary with the following format:\n",
        "\n",
        "```python\n",
        "{\n",
        "    'Title': 'When will the coronavirus outbreak peak?' #str\n",
        "    'Author': ['David Cyranoski'] # list, a list of author names in the same order as they appear on the page\n",
        "    'Published Date': '2020-04-21' # str, yyyy-mm-dd\n",
        "    'Summary': '.....' #str, the summary div between the title and author fields, or empty string if no summary is available\n",
        "    'Content': '.....' #list, the whole article content, where every element is a paragraph (i.e., comes from a <p> tag)\n",
        "}\n",
        "```\n",
        "\n",
        "The values of `Summary` and `Content` should be raw texts that do not contain any HTML tag. For example, if the input HTML code is `\"<p><b>Hello</b><a href=\"https://google.com\">World</a><p>\"` then the output should be `\"Hello World\"`.\n",
        "\n",
        "In the local test we have provided the full reference JSON files of the given article pages for you. If your dictionary does not match the reference JSON, you should print out both and do a careful comparison to see where the difference is.\n",
        "\n",
        "**Notes**:\n",
        "* Occasionally there are some \"Related\" blocks embedded in the article text (example [here](http://clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-domain-data-preparation/nature_related.png)). These are characterized by the `aside` HTML tag and should not be included.\n",
        "* The `Published Date` field should be the original article date, not the updated date. For example, the `Published Date` for [this article](https://www.nature.com/articles/d41586-020-00166-6) is 2020-01-22.\n",
        "* Remember to call `strip()` on all values in the returned dictionary so that there is no leading or trailing space anywhere. If a content paragraph becomes empty after `strip()`, it should not be included. You do not need to call any other text processing task in section A."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE4g4eOTm5CN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_page_nature(url):\n",
        "    \"\"\"\n",
        "    Parse a single New York Times article at the given URL\n",
        "    \n",
        "    args:\n",
        "        url (str) : the article URL\n",
        "    \n",
        "    return:\n",
        "        Dict[str, str] : the parsed information stored in JSON format, which includes:\n",
        "            Title, Author, Published Date, Summary and Content\n",
        "    \"\"\"\n",
        "    soup = retrieve_url(url)\n",
        "    for x in soup.select('aside'):\n",
        "      x.extract()\n",
        "    parsed_page = {}\n",
        "    parsed_page['Title'] = soup.select(\"h1.article-item__title\")[0].get_text().strip()\n",
        "    parsed_page['Author'] = [author.get_text().strip().replace(\" &\", \"\") for author in soup.select(\"h3.sans-serif.strong.tab.tab-skin.ma0\")]\n",
        "    parsed_page['Published Date'] = datetime.strptime(soup.select(\"time\", itemprop=\"datePublished\")[0].get_text(), '%d %B %Y').strftime(\"%Y-%m-%d\")\n",
        "    parsed_page['Summary'] = soup.select(\"div.article-item__teaser-text\")[0].get_text().strip()\n",
        "    parsed_page['Content'] = [element.get_text().strip() for element in soup.select(\"div.article__body p\") if (element.get_text() != \"\\n\" and element.get_text().strip())]\n",
        "    return parsed_page\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "tags": [
          "excluded_from_script"
        ],
        "id": "A_NtSpT0m5CP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9513610-c670-44ac-b491-4c0f9f38f95b"
      },
      "source": [
        "def test_parse_page_nature():\n",
        "    nature1 = parse_page_nature(\"https://www.nature.com/articles/d41586-020-00190-6\")\n",
        "    nature1_reference = json.load(open(\"local_test_refs/nature1.txt\"))\n",
        "    assert nature1 == nature1_reference\n",
        "    nature2 = parse_page_nature(\"https://www.nature.com/articles/d41586-020-00166-6\")\n",
        "    nature2_reference = json.load(open(\"local_test_refs/nature2.txt\"))\n",
        "    assert nature2 == nature2_reference\n",
        "    nature3 = parse_page_nature(\"https://www.nature.com/articles/d41586-020-00798-8\")\n",
        "    nature3_reference = json.load(open(\"local_test_refs/nature3.txt\"))\n",
        "    assert nature3 == nature3_reference\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_parse_page_nature()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g8Q2H5om5CU",
        "colab_type": "text"
      },
      "source": [
        "### Question 6: Parsing several Nature articles from a search page\n",
        "Now that you have successfully parsed individual article pages, the next step is to search for all the relevant articles and parse each of them. More specifically, we want to search for articles that:\n",
        "1. contain the term \"coronavirus\" (case-insensitive) *in their titles*\n",
        "1. do not contain any of the terms \"Daily briefing\", \"Podcast\" or \"Backchat\" (case-insensitive) in their titles\n",
        "1. were published in a given period of time (e.g., February 01, 2020 to March 01, 2020 inclusive)\n",
        "1. have \"News\" as the Article type and belong to the journal \"Nature\" -- these criteria can be specified in the search result page.\n",
        "\n",
        "Explore the [Nature search page](https://www.nature.com/search) and [Advanced search page](https://www.nature.com/search/advanced) to see how you may obtain the search results that satisfy the above conditions. Then implement the function `extract_nature_articles` that returns a list of JSON-like dictionaries, each resulting from calling `parse_page_nature` on one article in the search results.\n",
        "\n",
        "**Notes**:\n",
        "* The article dictionaries should be ordered based on their associated dates **on the search result page**, from earlier to later. If two articles have the same date, order them alphabetically based on their titles.\n",
        "* If you see a connection reset error message, your code is sending requests too fast. You can include a `time.sleep` for a short duration between requests to avoid this issue.\n",
        "* Don't use datetime.fromisoformat since it's only available from Python 3.7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yif5iMSMm5CV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_nature_articles(start_date, end_date, base_url = \"https://www.nature.com\"):\n",
        "    \"\"\"\n",
        "    Search for and parse all coronavirus-related News article from the Nature journal that were\n",
        "    published in a given period\n",
        "    \n",
        "    args:\n",
        "        start_date (str): the lower bound of the date range to filter articles,\n",
        "            has the format yyyy-mm-dd\n",
        "        end_date (str): the upper bound (inclusive) of the date range to filter articles,\n",
        "            has the format yyyy-mm-dd\n",
        "    \n",
        "    return:\n",
        "        List[Dict[str, str]] : a list of parsed JSON for each articles returned by\n",
        "            the search query\n",
        "    \"\"\"\n",
        "    from operator import itemgetter\n",
        "    # Get new url from manual experiment\n",
        "    keyword = \"coronavirus\"\n",
        "    journal = \"nature\"\n",
        "    article_type = \"news\"\n",
        "    base_url = \"https://www.nature.com\"\n",
        "    url = base_url + \"/search?title={0}&order=date_asc&journal={1}&article_type={2}\".format(keyword, journal, article_type) # date sorted\n",
        "    num_pages = int(retrieve_url(url).select(\"li.inline-group-item.inline-group-middle\")[-2].get_text().strip().replace(\"page \", \"\"))\n",
        "\n",
        "    # Iterate through pages\n",
        "    results = []\n",
        "    reach_max_date = False\n",
        "    for page_number in range(1, num_pages+1):\n",
        "      current_url = url + \"&page={0}\".format(page_number)\n",
        "      soup = retrieve_url(current_url)\n",
        "      if reach_max_date:\n",
        "        break\n",
        "      # Retrieve a list of docs in this page\n",
        "      for article in soup.select('h2 a', href=True):\n",
        "        json = parse_page_nature(base_url + article['href'])\n",
        "        article_time = datetime.strptime(json[\"Published Date\"], \"%Y-%m-%d\") \n",
        "        title = json[\"Title\"].lower()\n",
        "        # filter conditions: \n",
        "        # do not contain any of the terms \"Daily briefing\", \"Podcast\" or \"Backchat\" (case-insensitive) in their titles\n",
        "        # start and end date\n",
        "        if article_time > datetime.strptime(end_date, \"%Y-%m-%d\"):\n",
        "          reach_max_date = True\n",
        "          break\n",
        "        if \"Daily briefing\".lower() in title or \"Podcast\".lower() in title or \"Backchat\".lower() in title:\n",
        "          continue\n",
        "        if article_time < datetime.strptime(start_date, \"%Y-%m-%d\"):\n",
        "          continue\n",
        "        results.append(json)\n",
        "    # same date rank alphabetically\n",
        "    return sorted(results, key=itemgetter('Published Date', 'Title'))\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "excluded_from_script"
        ],
        "id": "pO1zo0gem5CX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99c935f5-f6bc-4448-e9f9-1191c10d3f68"
      },
      "source": [
        "def test_extract_nature_articles():\n",
        "    parsed_articles = extract_nature_articles(\"2020-02-01\", \"2020-03-01\")\n",
        "    expected_titles = [\n",
        "        'HIV vaccine failure, coronavirus papers and an unprecedented glimpse of the Sun',\n",
        "        'Did pangolins spread the China coronavirus to people?',\n",
        "        'How scientists are fighting the novel coronavirus: A three minute guide',\n",
        "        'CRISPR enhancement, coronavirus source and a controversial appointment',\n",
        "        'Scientists fear coronavirus spread in countries least able to contain it',\n",
        "        'More than 80 clinical trials launch to test coronavirus treatments',\n",
        "        'When will the coronavirus outbreak peak?',\n",
        "        'Coronavirus name, animal-research data and a Solar System snowman',\n",
        "        'Scientists question China‚Äôs decision not to report symptom-free coronavirus cases',\n",
        "        'China set to clamp down permanently on wildlife trade in wake of coronavirus',\n",
        "        '‚ÄòNo one is allowed to go out‚Äô: your stories from the coronavirus outbreak',\n",
        "        'Time to use the p-word? Coronavirus enters dangerous new phase',\n",
        "        'Mystery deepens over animal source of coronavirus'\n",
        "    ]\n",
        "    expected_dates = [\n",
        "        '2020-02-05', '2020-02-07', '2020-02-07', '2020-02-12',\n",
        "        '2020-02-13', '2020-02-15', '2020-02-18', '2020-02-19',\n",
        "        '2020-02-20', '2020-02-21', '2020-02-21', '2020-02-25', '2020-02-26'\n",
        "    ]\n",
        "    \n",
        "    assert len(parsed_articles) == 13\n",
        "    assert [a[\"Title\"] for a in parsed_articles] == expected_titles\n",
        "    assert [a[\"Published Date\"] for a in parsed_articles] == expected_dates\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_extract_nature_articles()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YafYLgJm5CZ",
        "colab_type": "text"
      },
      "source": [
        "Now we move to the second news source: The New York Times. You will perform a similar article searching and parsing process as above, but keep in mind that the website structure is different. If you are asked to subscribe / log in while trying to access an NYT article from your browser, refer to [Section 3.4 in the primer](https://nbviewer.jupyter.org/url/clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-data-collection-extraction-primer/data_collection_extraction_primer.ipynb) on resetting your cookies. This is not a problem if you use the `headless` option on your webdriver."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4uzUDdKm5Ca",
        "colab_type": "text"
      },
      "source": [
        "### Question 7: Parsing a single article from the New York Times\n",
        "\n",
        "Implement the function `parse_page_nyt` that takes as input an NYT article string URL and return a JSON dictionary with the following format:\n",
        "\n",
        "```python\n",
        "{\n",
        "    'Title': 'F.D.A. Approves First Coronavirus Antibody Test in U.S.' #str\n",
        "    'Author': ['Katie Thomas', 'Natasha Singer'] # list, a list of author names in the same order as they appear on the page\n",
        "    'Published Date': '2020-04-21' # str, yyyy-mm-dd\n",
        "    'Summary': '.....' #str, the summary paragraph between the title and author fields, or empty string if no summary is available\n",
        "    'Content': '.....' #list, the whole article content, where every element is a paragraph (i.e., comes from a <p> tag)\n",
        "}\n",
        "```\n",
        "\n",
        "The values of `Summary` and `Content` should be raw text that do not contain any HTML tag. For example, if the input HTML code is `\"<p><b>Hello</b> <a href=\"https://google.com\">World</a><p>\"` then the output should be `\"Hello World\"`.\n",
        "\n",
        "In the local test we have provided the full reference JSON files for you. If your dictionary does not match the reference JSON, you should print out both and do a careful comparison to see where the difference is.\n",
        "\n",
        "**Notes**:\n",
        "* Ignore the \"Frequently Asked Questions and Advice\" box (example [here](http://clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-domain-data-preparation/nyt_faq.png)).\n",
        "* You may find unicode characters, e.g., `\\u201d`, while parsing the page. It is fine to include them in the output.\n",
        "* The summary can be extracted from a `<p>` tag whose id is `article-summary`. If this tag is not present, simply use the empty string as the value.\n",
        "* Remember to call `strip()` on all values in the returned dictionary so that there is no leading or trailing space anywhere. If a content paragraph becomes empty after `strip()`, it should not be included. You do not need to call any other text processing task in section A."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydHvZvvhm5Ca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_page_nyt(url):\n",
        "    \"\"\"\n",
        "    Parse a New York Times article page to extract the title, authors, date, summary and content.\n",
        "    \n",
        "    args:\n",
        "        url (str) : the article page's URL\n",
        "    \n",
        "    return:\n",
        "        Dict[str, str] : a JSON-like dictionary whose keys are \"Title\", \"Author\", \"Published Date\", \"Summary\" and \"Content\"\n",
        "    \"\"\"\n",
        "    soup = retrieve_url(url)\n",
        "    for x in soup.select('div.g-inlineguide'):\n",
        "      x.extract()\n",
        "    parsed_page = {}\n",
        "    parsed_page['Title'] = soup.select(\"h1\", params={\"itemprop\" : \"headline\"})[0].get_text().strip()\n",
        "    parsed_page['Author'] = [author.get_text().strip().replace(\" &\", \"\") for author in soup.select(\"p span\", params={\"itemprop\" : \"author\"})]\n",
        "    parsed_page['Published Date'] = datetime.strptime(soup.select(\"time\", itemprop=\"datePublished\")[0].get_text().strip().replace(\"Published \", \"\"), '%B %d, %Y').strftime(\"%Y-%m-%d\")\n",
        "    parsed_page['Summary'] = soup.select(\"p#article-summary\")[0].get_text().strip()\n",
        "    parsed_page['Content'] = [element.get_text().strip() for element in soup.select(\"div.StoryBodyCompanionColumn p\") if (element.get_text() != \"\\n\" and element.get_text().strip())]\n",
        "    return parsed_page\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "tags": [
          "excluded_from_script"
        ],
        "id": "24H8vi3em5Cc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48a07776-c4e3-47d4-83a2-e778fecf8911"
      },
      "source": [
        "def test_parse_page_nyt():\n",
        "    nyt1 = parse_page_nyt(\"https://web.archive.org/web/20200603034948/https://www.nytimes.com/2020/04/21/health/fda-in-home-test-coronavirus.html\")\n",
        "    nyt1_reference = json.load(open(\"local_test_refs/nyt1.txt\"))\n",
        "    assert nyt1 == nyt1_reference\n",
        "    nyt2 = parse_page_nyt(\"https://web.archive.org/web/20200602022004/https://www.nytimes.com/2020/04/18/health/kidney-dialysis-coronavirus.html\")\n",
        "    nyt2_reference = json.load(open(\"local_test_refs/nyt2.txt\"))\n",
        "    assert nyt2 == nyt2_reference\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_parse_page_nyt()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haR71ZLBm5Ce",
        "colab_type": "text"
      },
      "source": [
        "### Question 8: Parsing several NYC articles from a search page\n",
        "\n",
        "Now that you have successfully parsed individual article pages, the next step is to search for all the relevant articles. More specifically, we want to search for articles that:\n",
        "1. are presented by entering the term \"coronavirus\" in the search box.\n",
        "1. were published in a given period of time (e.g., February 01, 2020 to March 01, 2020 inclusive)\n",
        "1. have \"Article\" as the type and belong to the \"Health\" section  -- these criteria can be specified in the search result page.\n",
        "\n",
        "Explore the [NYT search page](https://www.nytimes.com/search) to see how you may obtain the search results that satisfy the above conditions. Then implement the function `extract_nyt_articles` that returns the list of all article titles that appear on the search result page. **Because the html contents of NYT articles may change in real time, you only need to get the page titles in this case**. \n",
        "\n",
        "**Notes**:\n",
        "* The article titles should be ordered based on their associated dates **on the search result page**, from earlier to later. If two articles have the same date, order them alphabetically based on their titles.\n",
        "* Unlike the Nature search page which partitions the search results into several webpages, each with a unique URL, NYT includes all the search results in one webpage. However, initially only the first 10 results are visible; you need to click on the \"SHOW MORE\" button to see the next 10 results, then click again for the next 10, and so on. Use Selenium to perform this task automatically. Remember to pause the code briefly between clicks to give the website time to load new results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3of5Cvvm5Cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_nyt_articles(start_date, end_date, base_url = \"https://www.nytimes.com/\"):\n",
        "    \"\"\"\n",
        "    Retrieve all of the article that are shown on a given search result page and parse their contents.\n",
        "\n",
        "    args:\n",
        "    kwargs:\n",
        "        base_url (string) : the NYT home page URL \n",
        "\n",
        "    return:\n",
        "        List[str] : the list of article titles on the search result page,\n",
        "            ordered by associated dates and by name \n",
        "    \"\"\"\n",
        "    from operator import itemgetter\n",
        "    # Construct search url\n",
        "    start_date = start_date.replace(\"-\", \"\")\n",
        "    end_date = end_date.replace(\"-\", \"\")\n",
        "    search_url = base_url + \"search?&endDate=\" \\\n",
        "                          + end_date \\\n",
        "                          + \"&query=coronavirus&sections=Health%7Cnyt%3A%2F%2Fsection%2F9f943015-a899-5505-8730-6d30ed861520&&sort=oldest&startDate=\" \\\n",
        "                          + start_date \\\n",
        "                          + \"&types=article\"\n",
        "    # Selenium web driver for \"next page\"\n",
        "    driver = init_chromedriver()\n",
        "    driver.get(search_url)\n",
        "    while True:\n",
        "      try:\n",
        "        content = driver.find_element_by_css_selector(\"button[data-testid=\\\"%s\\\"]\"% \"search-show-more-button\")\n",
        "        content.click()\n",
        "        time.sleep(1)\n",
        "      except:\n",
        "        break\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "    # Sort based on both dates and title\n",
        "    titles = list([element.get_text().strip() for element in soup.select(\"a h4\")])\n",
        "    dates = [] # Expand the loop to cope with \"Feb. 29\" and \"March 3\" format issues\n",
        "    for element in soup.select(\"li div time\", params={\"data-testid\" : \"todays-date\"}):\n",
        "      try:\n",
        "        dates.append(datetime.strftime(datetime.strptime(element.get_text().replace(\". \", \" \"), \"%b %d\"), \"%m-%d\"))\n",
        "      except:\n",
        "        try:\n",
        "          dates.append(datetime.strftime(datetime.strptime(element.get_text().replace(\". \", \" \"), \"%B %d\"), \"%m-%d\"))\n",
        "        except:\n",
        "          dates.append('02-29')\n",
        "    result = []\n",
        "    for element in zip(titles, dates):\n",
        "      title, date = element\n",
        "      result.append({\"Title\" : title, \"Date\": date})\n",
        "    sorted_dict = sorted(result, key=itemgetter('Date', 'Title'))\n",
        "    return [x[\"Title\"] for x in sorted_dict]\n",
        "    "
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "excluded_from_script"
        ],
        "id": "wJhm10Y5m5Ch",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "600ce274-2592-4480-c121-1eaf4b326aba"
      },
      "source": [
        "def test_extract_nyt_articles():\n",
        "    nyt_articles = extract_nyt_articles(\"2020-02-01\", \"2020-03-01\")\n",
        "    expected_titles = [\n",
        "        'Wuhan Coronavirus Looks Increasingly Like a Pandemic, Experts Say',\n",
        "        'Even Without Symptoms, Wuhan Coronavirus May Spread, Experts Fear',\n",
        "        'Why the New Coronavirus (Mostly) Spares Children',\n",
        "        'China Begins Testing an Antiviral Drug in Coronavirus Patients',\n",
        "        'W.H.O. Fights a Pandemic Besides Coronavirus: An ‚ÄòInfodemic‚Äô',\n",
        "        'C.D.C. and W.H.O. Offers to Help China Have Been Ignored for Weeks',\n",
        "        'Inundated With Flu Patients, U.S. Hospitals Brace for Coronavirus',\n",
        "        'New Report on 138 Coronavirus Cases Reveals Disturbing Details',\n",
        "        'Huge Shelters for Coronavirus Patients Pose New Risks, Experts Fear',\n",
        "        'Coronavirus Cases Seemed to Be Leveling Off. Not Anymore.',\n",
        "        'Coronavirus Test Kits Sent to States Are Flawed, C.D.C. Says',\n",
        "        'Video Chats and Ordering In: Coronavirus Quarantine With a Smartphone',\n",
        "        'To Prevent Next Coronavirus, Stop the Wildlife Trade, Conservationists Say',\n",
        "        'Why the Coronavirus Seems to Hit Men Harder Than Women',\n",
        "        'Coronavirus Cases in the United States Reach 34, and More Are Expected',\n",
        "        'New Genomic Tests Aim to Diagnose Deadly Infections Faster',\n",
        "        'Westerdam Passengers at Low Risk of Coronavirus Infection, C.D.C. Says',\n",
        "        'C.D.C. Officials Warn of Coronavirus Outbreaks in the U.S.',\n",
        "        'C.D.C. Confirms First Possible Community Transmission of Coronavirus in U.S.',\n",
        "        'Gilead to Expand Coronavirus Drug Trials to Other Countries',\n",
        "        'They Were Infected With the Coronavirus. They Never Showed Signs.',\n",
        "        'Coronavirus Diagnosis in California Highlights Testing Flaws',\n",
        "        'What Has Mike Pence Done in Health?',\n",
        "        'First Drug Shortage Caused by Coronavirus, F.D.A. Says. But It Won‚Äôt Disclose What Drug or Where It‚Äôs Made.',\n",
        "        'Is the Coronavirus an Epidemic or a Pandemic? It Depends on Who‚Äôs Talking',\n",
        "        'When an Epidemic Looms, Gagging Scientists Is a Terrible Idea',\n",
        "        'How Prepared Is the U.S. for a Coronavirus Outbreak?',\n",
        "        'Surgeon General Urges the Public to Stop Buying Face Masks',\n",
        "        'They Recovered From the Coronavirus. Were They Infected Again?',\n",
        "        'U.S. Plans ‚ÄòRadical Expansion‚Äô of Coronavirus Testing',\n",
        "        'Who‚Äôs on the U.S. Coronavirus Task Force',\n",
        "        'Coronavirus May Have Spread in U.S. for Weeks, Gene Sequencing Suggests',\n",
        "        'How to Protect Yourself and Prepare for the Coronavirus'\n",
        "    ]\n",
        "    \n",
        "    assert nyt_articles == expected_titles\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_extract_nyt_articles()"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UybaokvSm5Cj",
        "colab_type": "text"
      },
      "source": [
        "### Question 9: Process news articles data\n",
        "While the JSON data format we constructed earlier is useful for checking the correctness of our parsing, eventually we would like each article to be represented by just a string, one that we can input to `preprocess_text` and get a list of processed tokens. For our purpose, we will define the string representation of an article as\n",
        "\n",
        "`\"<title> <summary> <content paragraph 1> <content paragraph 2> <content paragraph 3> ...\"`\n",
        "\n",
        "where there is a single space separating each field (note that the content paragraphs come from the `\"Content\"` field of an article json, which is a list of paragraph strings).\n",
        "\n",
        "Implement the function `process_news_article` that takes as input a JSON dictionary resulting from parsing a Nature and NYT article, converts the JSON to the above string format, and outputs a list of processed tokens from the article string.\n",
        "\n",
        "**Note**:\n",
        "* Remember to specify the stopwords parameter as the `english_stopwords` you downloaded when you call `preprocess_text`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVFeHa-Xm5Ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_news_article(article_json):\n",
        "    \"\"\"\n",
        "    Convert article jsons to nested list of tokens of processed article contents\n",
        "    \n",
        "    args:\n",
        "        article_json (Dict[str, str]] : JSON content of a news particle\n",
        "    \n",
        "    return:\n",
        "        List[str] : a list of processed tokens from the input article JSON\n",
        "    \"\"\"\n",
        "    result = []\n",
        "\n",
        "    # Process title and summary\n",
        "    result += preprocess_text(article_json['Title'], stopwords=english_stopwords)\n",
        "    result += preprocess_text(article_json['Summary'], stopwords=english_stopwords)\n",
        "\n",
        "    #Process paragraphs\n",
        "    for paragraph in article_json['Content']:\n",
        "      result += preprocess_text(paragraph, stopwords=english_stopwords)\n",
        "    return result\n",
        "\n",
        "# do not modify this function\n",
        "def process_news_articles_data(article_jsons):\n",
        "    return [process_news_article(article) for article in article_jsons]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "excluded_from_script"
        ],
        "id": "4XHvam5Qm5Cm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81721b76-51d5-41f2-9890-b574a859d3de"
      },
      "source": [
        "def test_process_news_article():\n",
        "    nature_article = json.load(open(\"local_test_refs/nature1.txt\"))   \n",
        "    nature_article_processed = process_news_article(nature_article)\n",
        "    nature_expected = open(\"local_test_refs/nature1_processed.txt\").read().splitlines()\n",
        "    assert nature_article_processed == nature_expected\n",
        "    \n",
        "    nyt_article = json.load(open(\"local_test_refs/nyt1.txt\"))\n",
        "    nyt_article_processed = process_news_article(nyt_article)\n",
        "    nyt_expected = open(\"local_test_refs/nyt1_processed.txt\").read().splitlines()\n",
        "    assert nyt_article_processed == nyt_expected\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_process_news_article()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXEXOzYXm5Cp",
        "colab_type": "text"
      },
      "source": [
        "## Part D: Mining PDF Data\n",
        "Having extracted data from Twitter and newspapers, we now turn to our third source: research papers. More specifically, we will collect research papers from https://arxiv.org/ using the [arxiv API](https://arxiv.org/help/api). As the search result from arxiv may change in real time, we have provided you with 15 pdf files collected from arxiv to extract text from. These are located in the `pdfs` directory and labeled from `arxiv_01.pdf` to `arxiv_15.pdf`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLleFixXm5Cp",
        "colab_type": "text"
      },
      "source": [
        "### Question 10: Parse a single Arxiv research paper\n",
        "Implement the function `parse_pdf` that takes as input a PDF file path and outputs the processed tokenization of the text content of that file. In particular, you should perform the following steps:\n",
        "\n",
        "1. Remove all URLs, i.e., strings that start with \"http://\" or \"https://\"\n",
        "1. Call the `preprocess_text` function you implemented in part A. Remember to specify the `stopwords` parameter.\n",
        "\n",
        "**Notes**:\n",
        "* For this question, you should use the function [`extract_text`](https://pdfminersix.readthedocs.io/en/latest/reference/highlevel.html#extract-text) from the `pdfminer` package to convert a pdf file to string.\n",
        "* Unlike in the tweet scenario, there is no limit on the length of an URL in this case. You can assume that an URL ends when there is any space character (e.g., single space, newline, tab). For example, when parsing `\"http://example.com abc\"`, you can assume that the URL portion stops before the space, i.e., it is `http://example.com`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Na_h28lwm5Cq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_and_clean_pdf(file):\n",
        "    \"\"\"\n",
        "    Convert an input pdf file into processed and cleaned raw text.\n",
        "    \n",
        "    args:\n",
        "        file (str) : the pdf file path\n",
        "    \n",
        "    return:\n",
        "        List[str] : the cleaned tokenization of the input file content\n",
        "    \"\"\"\n",
        "    text = high_level.extract_text(file)\n",
        "    text = re.sub(\"(?:https|http)\\:\\/\\/(\\S+)\", \"\", text)\n",
        "    return preprocess_text(text, stopwords=english_stopwords)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "excluded_from_script"
        ],
        "id": "eqpAepK0m5Cs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4cf5493e-6359-49e1-ecf4-a734597f6000"
      },
      "source": [
        "def test_parse_and_clean_pdf():\n",
        "    pdf_text = parse_and_clean_pdf(\"pdfs/arxiv_01.pdf\")\n",
        "    with open(\"local_test_refs/parsed_arxiv_01.txt\") as outfile:\n",
        "        assert pdf_text == outfile.read().splitlines()\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_parse_and_clean_pdf()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja8oy-dAm5Cu",
        "colab_type": "text"
      },
      "source": [
        "### Question 11: Parse several Arxiv research papers\n",
        "Implement the function `process_arxiv_data` that takes as input the path to a directory. This function parses and cleans all pdf files in that directory, then returns a nested list of word tokens, where each inner list results from parsing one PDF file.\n",
        "\n",
        "**Notes**:\n",
        "* The pdf files should be processed based on the **alphabetical order** of their name, e.g., `arxiv_01.pdf` before `arxiv_02.pdf`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcsWL4HTm5Cv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_arxiv_data(directory):\n",
        "    \"\"\"\n",
        "    Parse and process the text content of all pdf papers in alphabetical order in a given directory\n",
        "    \n",
        "    args:\n",
        "        directory (str) : the relative file path to a directory that contains the pdf papers\n",
        "    \n",
        "    return:\n",
        "        List[List[str]] : a list of list of word tokens\n",
        "    \"\"\"\n",
        "    def get_all_files_from_dir(directory):\n",
        "      file_paths = []\n",
        "      for root, dirs, files in os.walk(directory):\n",
        "          file_paths += [os.path.join(root, x) for x in files]\n",
        "      return sorted(file_paths)\n",
        "\n",
        "    results = []\n",
        "    file_names = get_all_files_from_dir(directory)\n",
        "    for file in file_names:\n",
        "      if file.split(\".\")[-1] == \"pdf\":\n",
        "        results.append(parse_and_clean_pdf(file))\n",
        "    return results\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "excluded_from_script"
        ],
        "id": "NQIi_aqBm5Cy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23f9b8a2-df9f-4ee8-8f9e-625409b03718"
      },
      "source": [
        "def test_process_arxiv_data():\n",
        "    paper_contents = process_arxiv_data(\"pdfs\")\n",
        "    first_words = [paper[0] for paper in paper_contents]\n",
        "    mid_words = [paper[100] for paper in paper_contents]\n",
        "    last_words = [paper[-1] for paper in paper_contents]\n",
        "    \n",
        "    assert len(paper_contents) == 15\n",
        "    assert first_words == [\n",
        "        'repurposed', 'fractal', 'coronavirus', 'data', 'xu1', 'parametric',\n",
        "        'view', 'insight', 'reconstruction', 'covid', 'outbreak', 'scale',\n",
        "        'abnormal', 'trend', 'deep'\n",
        "    ]\n",
        "    assert mid_words == [\n",
        "        'result', 'nal', 'ro', 'february', 'covid', 'sars',\n",
        "        'virus', 'export', 'system', 'new', 'transmission', 'cantly',\n",
        "        'signi', 'content', 'wenling'\n",
        "    ]\n",
        "    assert last_words == [\n",
        "        '2019', '13', '122567', '13', 'lancet', 'url',\n",
        "        '77', 'hour', '330', '16', 'provide', '2004',\n",
        "        '212', '2020', '29'\n",
        "    ]\n",
        "    assert sum(len(paper) for paper in paper_contents) == 35969\n",
        "    print(\"All tests passed!\")\n",
        "    \n",
        "test_process_arxiv_data()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAIRMV2om5C0",
        "colab_type": "text"
      },
      "source": [
        "## Part E: Data Visualization and Feature Construction\n",
        "Now that we have collected text data from three different sources (Twitter, news articles and research papers), let's put them all together in order to perform some simple exploratory data analyses and feature construction. From now we will define a *document* as a list of tokens coming from a single tweet, news article or arxiv paper, and a *corpus* as a list of documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhegSCkNm5C1",
        "colab_type": "text"
      },
      "source": [
        "### Question 12: Word frequency and word cloud\n",
        "With any text corpus, you will first want to check for the word frequency distribution, in particular which words are the most common and which are the least. The former group may consist of terms that are relevant to the topic, or terms that simply appear frequently in general (e.g., stopwords). The latter group may consist of highly specialized terms or typos. Since stopwords and rare words are not useful to our analysis, we will remove both (where we define rare words as words that only appear *once in the corpus*).\n",
        "\n",
        "Implement the function `word_frequency` which takes as input a text corpus and returns a `collections.Counter` object mapping each word to its frequency in the corpus. However, rare words that only appear once in the entire corpus should **not** be included in this mapping.\n",
        "\n",
        "**Notes**:\n",
        "* Recall that `preprocess_text` already handles stopword removal, so you only need to remove rare words in this step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbZTwok4m5C1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_frequency(corpus):\n",
        "    \"\"\"\n",
        "    Count the word frequency in a given corpus\n",
        "    \n",
        "    args:\n",
        "        corpus (List[List[str]]) : a nested list of tokens, where each inner list is a processed document\n",
        "    \n",
        "    return:\n",
        "        collections.Counter : a mapping between each word and its frequency in the corpus, excluding words that\n",
        "            only appear once\n",
        "    \"\"\"\n",
        "    flatten = [token for doc in corpus for token in doc]\n",
        "    full_count = collections.Counter(flatten)\n",
        "    return collections.Counter(key for key in full_count.elements() if full_count[key] > 1)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "excluded_from_script"
        ],
        "id": "DDEKkFtNm5C3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f0f6351-f16e-4af0-9548-47a536639097"
      },
      "source": [
        "def test_word_frequency():\n",
        "    tweet_corpus = process_tweet_data(\n",
        "        get_tweet_texts_with_params(lang = \"en\", start = \"2020-03-09\", end = \"2020-03-09\", n_tweets = 1000)\n",
        "    )\n",
        "    counter = word_frequency(tweet_corpus)\n",
        "    assert len(counter) == 1739\n",
        "    assert counter[\"coronavirus\"] == 407\n",
        "    assert counter[\"coronavirusoutbreak\"] == 684\n",
        "    assert counter[\"increasingly\"] == 2\n",
        "    assert counter[\"stimulus\"] == 3\n",
        "    assert min(counter.values()) == 2\n",
        "    print(\"All tests passed!\")\n",
        "    \n",
        "test_word_frequency()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbNIY83Fm5C5",
        "colab_type": "text"
      },
      "source": [
        "Now we will gather all three corpuses together; we store them in a global cache to avoid having to construct them more than once. If you make any code change above this point, rerun the following cell to reset the cache."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3gt9Ce9m5C6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpuses = None\n",
        "\n",
        "def get_corpuses():\n",
        "    global corpuses\n",
        "    if corpuses is None:\n",
        "        twitter_corpus = process_tweet_data(\n",
        "            get_tweet_texts_with_params(lang = \"en\", start = \"2020-03-09\", end = \"2020-03-09\")\n",
        "        )\n",
        "        news_corpus = process_news_articles_data(\n",
        "            extract_nature_articles(\"2020-03-01\", \"2020-03-15\")\n",
        "        )\n",
        "        arxiv_corpus = process_arxiv_data(\"pdfs\")\n",
        "        corpuses = (twitter_corpus, news_corpus, arxiv_corpus)\n",
        "    return corpuses"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFd6OuX5m5C8",
        "colab_type": "text"
      },
      "source": [
        "Let's first compare the frequency of a number of keywords across these three corpuses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "excluded_from_script"
        ],
        "id": "Fe6pI29Wm5C8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "3cfaab3b-f0fc-4b47-e530-3e27137bb530"
      },
      "source": [
        "# this cell has been taggged with excluded_from_script\n",
        "# it will not be run by the autograder\n",
        "def get_word_frequency_across_corpuses(words):\n",
        "    twitter_corpus, news_corpus, arxiv_corpus = get_corpuses()\n",
        "    twitter_corpus_size = sum(len(d) for d in twitter_corpus)\n",
        "    news_corpus_size = sum(len(d) for d in news_corpus)\n",
        "    arxiv_corpus_size = sum(len(d) for d in arxiv_corpus)\n",
        "    twitter_f, news_f, arxiv_f = word_frequency(twitter_corpus), word_frequency(news_corpus), word_frequency(arxiv_corpus)\n",
        "    return pd.DataFrame({\n",
        "        \"Proportion in twitter corpus\" : [twitter_f.get(word, 0) / twitter_corpus_size for word in words],\n",
        "        \"Proportion in news corpus\" : [news_f.get(word, 0) / news_corpus_size for word in words],\n",
        "        \"Proportion in arxiv corpus\" : [arxiv_f.get(word, 0) / arxiv_corpus_size for word in words]\n",
        "    }, index = words)\n",
        "\n",
        "df_frequency = get_word_frequency_across_corpuses([\n",
        "    \"coronavirus\", \"covid\", \"case\", \"health\", \"model\", \"say\", \"test\",\n",
        "    \"2020\", \"19\", \"people\", \"vaccine\"\n",
        "])\n",
        "\n",
        "display(df_frequency)\n",
        "\n",
        "df_frequency.plot(kind='bar')"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Proportion in twitter corpus</th>\n",
              "      <th>Proportion in news corpus</th>\n",
              "      <th>Proportion in arxiv corpus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>coronavirus</th>\n",
              "      <td>0.027499</td>\n",
              "      <td>0.010319</td>\n",
              "      <td>0.004059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>covid</th>\n",
              "      <td>0.011581</td>\n",
              "      <td>0.004128</td>\n",
              "      <td>0.007284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>case</th>\n",
              "      <td>0.007141</td>\n",
              "      <td>0.004763</td>\n",
              "      <td>0.007923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>health</th>\n",
              "      <td>0.002728</td>\n",
              "      <td>0.003810</td>\n",
              "      <td>0.001779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <td>0.000128</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.010426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>say</th>\n",
              "      <td>0.003698</td>\n",
              "      <td>0.019051</td>\n",
              "      <td>0.000083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>0.004369</td>\n",
              "      <td>0.006191</td>\n",
              "      <td>0.001084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020</th>\n",
              "      <td>0.000945</td>\n",
              "      <td>0.000953</td>\n",
              "      <td>0.011343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.009409</td>\n",
              "      <td>0.003969</td>\n",
              "      <td>0.008257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>people</th>\n",
              "      <td>0.006611</td>\n",
              "      <td>0.007462</td>\n",
              "      <td>0.003447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>vaccine</th>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.001111</td>\n",
              "      <td>0.000056</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Proportion in twitter corpus  ...  Proportion in arxiv corpus\n",
              "coronavirus                      0.027499  ...                    0.004059\n",
              "covid                            0.011581  ...                    0.007284\n",
              "case                             0.007141  ...                    0.007923\n",
              "health                           0.002728  ...                    0.001779\n",
              "model                            0.000128  ...                    0.010426\n",
              "say                              0.003698  ...                    0.000083\n",
              "test                             0.004369  ...                    0.001084\n",
              "2020                             0.000945  ...                    0.011343\n",
              "19                               0.009409  ...                    0.008257\n",
              "people                           0.006611  ...                    0.003447\n",
              "vaccine                          0.000562  ...                    0.000056\n",
              "\n",
              "[11 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fcaf7582da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Jrct88m5DA",
        "colab_type": "text"
      },
      "source": [
        "We see that there are differences across datasets in the relative frequency of each term. \"Coronavirus\" is used most frequently in tweets, \"say\" most frequently in news corpus, and perhaps unsurprisingly, \"model\" most frequently in arxiv papers. The scientific notation of coronavirus, \"covid,\" isn't used in news articles as much, but is equally popular in both tweets and arxiv papers. On the other hand, \"health\" sees most frequent usage in news articles, likely due to health advice-related articles. Feel free to edit the word list above and see what other insights you can derive!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs4hLve2m5DA",
        "colab_type": "text"
      },
      "source": [
        "We now move to the last step of data collection and preparation: constructing input features to be used for more formal analyses and language modeling. As language modeling will be covered later in the course, here we will only cover two simple feature construction methods: term frequency (TF) and term frequency - inverse document frequency (TF-IDF)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFavxy7Pm5DB",
        "colab_type": "text"
      },
      "source": [
        "### Question 13: Feature construction: term frequency (TF)\n",
        "Implement the function `construct_tf_matrix` that takes as input a corpus and outputs a matrix $TF$ where each row corresponds to one document, and each column corresponds to one of the unique words in the entire corpus. $TF_{ij}$ is the number of times word $j$ appears in document $i$. Similar to the previous question, rare words that only appear once in the entire corpus should be removed, i.e., there should be no columns for those words.\n",
        "\n",
        "**Notes**:\n",
        "* The rows should be ordered based on the document ordering in the corpus. Row 0 corresponds to `corpus[0]`, row 1 to `corpus[1]`, and so on.\n",
        "* The columns should be ordered based on the alphebatical order of their corresponding words. Column 0 corresponds to the alphabetically first word in the corpus, column 1 to the alphabetically second word, and so on.\n",
        "* To ensure code efficiency, avoid using too many for-loops; take advantage of Pandas and Numpy functionalities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5VERrj09vOw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7e14937a-2fd2-490b-b3ee-94bc9c94aa82"
      },
      "source": [
        "def compute_TF_doc(doc, columns):\n",
        "      full_count = collections.Counter(doc)\n",
        "      return [full_count[x] for x in columns]\n",
        "\n",
        "corpus = [\n",
        "        \"this project is project 3 in foundations of computational data science\".split(),\n",
        "        \"it covers text data collection and preparation in the data science pipeline\".split(),\n",
        "        \"text processing can be tricky sometimes\".split()\n",
        "    ]\n",
        "col_sort = [x[0] for x in sorted(word_frequency(corpus).items())] # sort by key (word alphbetically)\n",
        "\n",
        "values = list(map(lambda doc, columns=col_sort: compute_TF_doc(doc,col_sort), corpus))\n",
        "df_matrix = pd.DataFrame(values, index=sorted(list(range(len(corpus)))), columns=col_sort)\n",
        "df_matrix.to_numpy()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 2, 1, 0],\n",
              "       [2, 1, 0, 1, 1],\n",
              "       [0, 0, 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBaNCSbtm5DC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_tf_matrix(corpus):\n",
        "    \"\"\"\n",
        "    Construct a term frequency matrix from an input corpus\n",
        "    \n",
        "    args:\n",
        "        corpus (List[List[str]]) : a nested list of word tokens, where each inner list is a document\n",
        "    \n",
        "    return:\n",
        "        np.array[n_documents, n_words] : the term frequency matrix\n",
        "    \"\"\"\n",
        "    def compute_TF_doc(doc, columns):\n",
        "      full_count = collections.Counter(doc)\n",
        "      return [full_count[x] for x in columns]\n",
        "    \n",
        "    # Remove low freq words and sort by key (word alphbetically)\n",
        "    col_sort = [x[0] for x in sorted(word_frequency(corpus).items())]\n",
        "    values = list(map(lambda doc, columns=col_sort: compute_TF_doc(doc, col_sort), corpus))\n",
        "    df_matrix = pd.DataFrame(values, index=sorted(list(range(len(corpus)))), columns=col_sort)\n",
        "    return df_matrix.to_numpy()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "excluded_from_script"
        ],
        "id": "gTtq56ukm5DE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "80348269-0a35-484d-c3f1-4af5eb7ddb7f"
      },
      "source": [
        "def test_construct_tf_matrix():\n",
        "    corpus = [\n",
        "        \"this project is project 3 in foundations of computational data science\".split(),\n",
        "        \"it covers text data collection and preparation in the data science pipeline\".split(),\n",
        "        \"text processing can be tricky sometimes\".split()\n",
        "    ]\n",
        "    tf = construct_tf_matrix(corpus)\n",
        "    assert (tf == np.array([\n",
        "        [1, 1, 2, 1, 0],\n",
        "        [2, 1, 0, 1, 1],\n",
        "        [0, 0, 0, 0, 1]]\n",
        "    )).all()\n",
        "    \n",
        "    twitter_corpus, news_corpus, arxiv_corpus = get_corpuses()\n",
        "    all_corpuses = twitter_corpus + news_corpus + arxiv_corpus\n",
        "    tf = construct_tf_matrix(all_corpuses)\n",
        "    assert tf.dtype == np.int64\n",
        "    assert tf.shape == (10025, 10011)\n",
        "    assert (tf.sum(axis = 1)[:10] == np.array([ 9, 14, 10, 17, 27,  4, 22, 23, 17, 13])).all()\n",
        "    assert (tf.sum(axis = 1)[-10:] == np.array([1465, 1935, 4683, 1857, 2096, 2498, 1995, 2324, 1811, 3370])).all()\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_construct_tf_matrix()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All tests passed!\n",
            "CPU times: user 1min 10s, sys: 659 ms, total: 1min 11s\n",
            "Wall time: 1min 12s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SiwtCCam5DH",
        "colab_type": "text"
      },
      "source": [
        "### Question 14: Feature construction: term frequency - inverse document frequency (TF-IDF)\n",
        "We can now compute the TF-IDF matrix, which scales the columns of the term frequency matrix by their inverse document frequency. Recall that the inverse document frequency of a word $j$ is computed as\n",
        "$$\\text{IDF}_j = \\log \\left( \\frac{\\# \\text{of documents}}{\\# \\text{of documents with word } j} \\right),$$\n",
        "and so the $\\text{TF-IDF}_{ij}$ entry in the tf-idf matrix is computed as\n",
        "$$\\text{TF-IDF}_{ij} = \\text{TF}_{ij} \\times \\text{IDF}_j.$$\n",
        "\n",
        "Implement the function `tf_idf_matrix` which takes as input a TF matrix and outputs the corresponding TF-IDF matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9AM3nSlm5DH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_tf_idf_matrix(tf_matrix):\n",
        "    \"\"\"\n",
        "    Compute the term frequency - inverse document frequency in a corpus\n",
        "    \n",
        "    args:\n",
        "        tf_matrix (np.array[n_documents, n_words]) : the term frequency document of the corpus\n",
        "    \n",
        "    return:\n",
        "        np.array[n_documents, n_words] : the tf-idf matrix\n",
        "    \"\"\"\n",
        "    num_documents = tf_matrix.shape[0]\n",
        "    idf_matrix = np.vstack([np.log(num_documents / tf_matrix.astype(bool).sum(axis=0))] * num_documents)\n",
        "    return np.multiply(tf_matrix, idf_matrix)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "excluded_from_script"
        ],
        "id": "GW4JqBMem5DJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "59145680-50b0-4411-820b-9d0d7194a54f"
      },
      "source": [
        "def test_construct_tf_idf_matrix():\n",
        "    corpus = [\n",
        "        \"this project is project 3 in foundations of computational data science\".split(),\n",
        "        \"it covers text data collection and preparation in the data science pipeline\".split(),\n",
        "        \"text processing can be tricky sometimes\".split()\n",
        "    ]\n",
        "    tf_idf = construct_tf_idf_matrix(construct_tf_matrix(corpus))\n",
        "    assert np.allclose(tf_idf, np.array([\n",
        "        [0.40546511, 0.40546511, 2.19722458, 0.40546511, 0.        ],\n",
        "        [0.81093022, 0.40546511, 0.        , 0.40546511, 0.40546511],\n",
        "        [0.        , 0.        , 0.        , 0.        , 0.40546511]\n",
        "    ]))\n",
        "    print(\"All tests passed!\")\n",
        "    \n",
        "test_construct_tf_idf_matrix()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zGM5Vc2m5DN",
        "colab_type": "text"
      },
      "source": [
        "# Do not run or modify the code below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAcPmYqJm5DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os.path\n",
        "\n",
        "# q1\n",
        "def grader_test_preprocess_text():\n",
        "    text = open(\"q1_preprocess_text_inputs.txt\", \"r\").read()\n",
        "    result = preprocess_text(text, english_stopwords)\n",
        "    print(result)\n",
        "    \n",
        "    \n",
        "# q2\n",
        "def grader_test_get_tweets():\n",
        "    _, result = get_tweets()\n",
        "    print(result)\n",
        "\n",
        "\n",
        "# q3\n",
        "def grader_test_get_tweet_texts_with_params():\n",
        "    lang, start, end, n = open(\"q3_get_tweet_texts_with_params_inputs.txt\", \"r\").read().splitlines()\n",
        "    result = get_tweet_texts_with_params(lang = lang, start = start, end = end, n_tweets = int(n))\n",
        "    pd.to_pickle(result, \"q3_get_tweet_texts_with_params.pkl\")\n",
        "    print(result)\n",
        "    \n",
        "\n",
        "# q4\n",
        "def grader_test_process_tweet_data():\n",
        "    tweet_texts = pd.read_pickle(\"q3_get_tweet_texts_with_params.pkl\")\n",
        "    result = process_tweet_data(tweet_texts)\n",
        "    pd.to_pickle(result,'q4_process_tweet_data.pkl')\n",
        "    print(result)\n",
        "    \n",
        "# q5\n",
        "def grader_test_parse_page_nature():\n",
        "    links = open(\"q5_parse_page_nature_inputs.txt\", \"r\").read().split(\"\\n\")\n",
        "    result = [parse_page_nature(link) for link in links]\n",
        "    print(result)\n",
        "\n",
        "    \n",
        "# q6\n",
        "def grader_test_extract_nature_articles():\n",
        "    start_date, end_date = open(\"q6_extract_nature_articles_inputs.txt\", \"r\").read().splitlines()\n",
        "    result = extract_nature_articles(start_date, end_date)\n",
        "    pd.to_pickle(result, 'q6_extract_nature_articles.pkl')\n",
        "    print(result) \n",
        "    \n",
        "\n",
        "# q7\n",
        "def grader_test_parse_page_nyt():\n",
        "    links = open(\"q7_parse_page_nyt_inputs.txt\").read().splitlines()\n",
        "    result = [parse_page_nyt(link) for link in links]\n",
        "    print(result)\n",
        "\n",
        "\n",
        "# q8\n",
        "def grader_test_extract_nyt_articles():\n",
        "    start_date, end_date = open(\"q8_extract_nyt_articles_inputs.txt\", \"r\").read().splitlines()\n",
        "    result = extract_nyt_articles(start_date, end_date)\n",
        "    pd.to_pickle(result, 'q8_extract_nyt_articles.pkl')\n",
        "    print(result)     \n",
        "    \n",
        "# q9\n",
        "def grader_test_process_news_articles_data():\n",
        "    articles = pd.read_pickle('q6_extract_nature_articles.pkl')\n",
        "    result = process_news_articles_data(articles)\n",
        "    pd.to_pickle(result, 'q9_process_news_articles_data.pkl')\n",
        "    print(result)         \n",
        "    \n",
        "\n",
        "# q10\n",
        "def grader_test_parse_and_clean_pdf():\n",
        "    result = parse_and_clean_pdf(\"pdfs/arxiv_11.pdf\")\n",
        "    print(result)      \n",
        "    \n",
        "\n",
        "# q11\n",
        "def grader_test_process_arxiv_data():\n",
        "    result = process_arxiv_data(\"pdfs\")\n",
        "    pd.to_pickle(result, 'q11_process_arxiv_data.pkl')\n",
        "    result = sorted(sum(result, []))\n",
        "    print(result) \n",
        "    \n",
        "# q12\n",
        "def grader_test_word_frequency():\n",
        "    tweet_corpus = pd.read_pickle('q4_process_tweet_data.pkl')\n",
        "    result = word_frequency(tweet_corpus)\n",
        "    print(result)\n",
        "    \n",
        "# q13\n",
        "def grader_test_construct_tf_matrix():\n",
        "    all_corpuses = (\n",
        "        pd.read_pickle(\"q4_process_tweet_data.pkl\"),\n",
        "        pd.read_pickle('q9_process_news_articles_data.pkl'),\n",
        "        pd.read_pickle('q11_process_arxiv_data.pkl')\n",
        "    )\n",
        "    result = construct_tf_matrix(all_corpuses[0] + all_corpuses[1] + all_corpuses[2])\n",
        "    np.savetxt(\"q13_construct_tf_matrix.npy\", result, fmt = \"%d\")\n",
        "    print(result)\n",
        "    \n",
        "# q14\n",
        "def grader_test_construct_tf_idf_matrix():\n",
        "    tf_matrix = np.loadtxt(\"q13_construct_tf_matrix.npy\", dtype = np.int64)\n",
        "    result = construct_tf_idf_matrix(tf_matrix)\n",
        "    print(result.round(10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haTF925mm5DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    global PATH_TO_CHROMEDRIVER\n",
        "    global PATH_TO_GECKODRIVER\n",
        "    PATH_TO_CHROMEDRIVER = \"/usr/bin/chromedriver\"\n",
        "    PATH_TO_GECKODRIVER = \"/usr/bin/geckodriver\"\n",
        "    # set description\n",
        "    parser = argparse.ArgumentParser(description=\"Data Preparation\")\n",
        "    parser.add_argument(\"-r\",\n",
        "                        metavar='<question_id>',\n",
        "                        required=False)\n",
        "    args = parser.parse_args()\n",
        "    question = args.r\n",
        "\n",
        "    if question is None:\n",
        "        grader_test_preprocess_text()\n",
        "        grader_test_get_tweets()\n",
        "        grader_test_get_tweet_texts_with_params()\n",
        "        grader_test_process_tweet_data()\n",
        "        grader_test_parse_page_nature()\n",
        "        grader_test_extract_nature_articles()\n",
        "        grader_test_parse_page_nyt()\n",
        "        grader_test_extract_nyt_articles()\n",
        "        grader_test_process_news_articles_data()\n",
        "        grader_test_parse_and_clean_pdf()\n",
        "        grader_test_process_arxiv_data()\n",
        "        grader_test_word_frequency()\n",
        "        grader_test_construct_tf_matrix()\n",
        "        grader_test_construct_tf_idf_matrix()\n",
        "    elif question == \"q1\":\n",
        "        grader_test_preprocess_text()\n",
        "    elif question == \"q2\":\n",
        "        grader_test_get_tweets()\n",
        "    elif question == \"q3\":\n",
        "        grader_test_get_tweet_texts_with_params()\n",
        "    elif question == \"q4\":\n",
        "        grader_test_process_tweet_data()\n",
        "    elif question == \"q5\":\n",
        "        grader_test_parse_page_nature()\n",
        "    elif question == \"q6\":\n",
        "        grader_test_extract_nature_articles()\n",
        "    elif question == \"q7\":\n",
        "        grader_test_parse_page_nyt()\n",
        "    elif question == \"q8\":\n",
        "        grader_test_extract_nyt_articles()\n",
        "    elif question == \"q9\":\n",
        "        grader_test_process_news_articles_data()\n",
        "    elif question == \"q10\":\n",
        "        grader_test_parse_and_clean_pdf()\n",
        "    elif question == \"q11\":\n",
        "        grader_test_process_arxiv_data()  \n",
        "    elif question == \"q12\":\n",
        "        grader_test_word_frequency()\n",
        "    elif question == \"q13\":\n",
        "        grader_test_construct_tf_matrix()\n",
        "    elif question == \"q14\":\n",
        "        grader_test_construct_tf_idf_matrix()\n",
        "    else:\n",
        "        print(\"Invalid question\")\n",
        "        \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}