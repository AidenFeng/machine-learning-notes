{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Collection and Extraction\n",
    "In this primer we will go over a number of techniques to gather and collect our own data from various sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using HTTP requests and API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. HTTP request basics\n",
    "Application programming interface (API) is a blueprint for communication between softwares; it defines, among others, the kind of requests that can be made, how to make them, as well as the data format that can be used. Many online services provide their own APIs for developers to request data from (for example, [Facebook](https://developers.facebook.com/docs/graph-api/), [Twitter](https://developer.twitter.com/en/docs) and [Yelp](https://www.yelp.com/developers)). A common way of interacting with these APIs is to send an HTTP GET request to the provided URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a request, we use `requests.get()` and provide the URL string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"https://api.github.com\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then examine different attributes of the `response` objects to see what is returned by our request. Here are some important attributes to pay attention to:\n",
    "\n",
    "* `status_code` indicates the HTTP status code of the request. There are many status codes that represent different response states -- you can see the full list on [Wikipedia](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes). Here is a handy summary table:\n",
    "\n",
    "<br>\n",
    "\n",
    "| Status code \t| Meaning \t| What the server is basically saying \t|\n",
    "|-\t|-\t|-\t|\n",
    "| 1xx - informational response \t| Request was received, continuing process \t| \"Hold on\" \t|\n",
    "| 2xx - successful \t| Request was successfully received and accepted \t| \"Here you go\" \t|\n",
    "| 3xx - redirect \t| Further action needs to be taken elsewhere \t| \"Go away\" \t|\n",
    "| 4xx - client error \t| The request cannot be fulfilled \t| \"You messed up\" \t|\n",
    "| 5xx - server error \t| The server failed to fulfill the request \t| \"I messed up :( \" \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally a 200 status code is what we would expect from a successful request. Since we did get 200 from our request, we can go head and examine other attributes:\n",
    "\n",
    "* `headers` contains meta-information about the page; this typically includes information not related to the main content that may be useful, e.g., server, content type, and cache control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'server': 'GitHub.com', 'date': 'Sun, 21 Jun 2020 07:10:30 GMT', 'content-type': 'application/json; charset=utf-8', 'status': '200 OK', 'cache-control': 'public, max-age=60, s-maxage=60', 'vary': 'Accept, Accept-Encoding, Accept, X-Requested-With', 'etag': 'W/\"c6bac8870a7f94b08b440c3d5873c9ca\"', 'x-github-media-type': 'github.v3; format=json', 'access-control-expose-headers': 'ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset', 'access-control-allow-origin': '*', 'strict-transport-security': 'max-age=31536000; includeSubdomains; preload', 'x-frame-options': 'deny', 'x-content-type-options': 'nosniff', 'x-xss-protection': '1; mode=block', 'referrer-policy': 'origin-when-cross-origin, strict-origin-when-cross-origin', 'content-security-policy': \"default-src 'none'\", 'content-encoding': 'gzip', 'X-Ratelimit-Limit': '60', 'X-Ratelimit-Remaining': '57', 'X-Ratelimit-Reset': '1592726165', 'Accept-Ranges': 'bytes', 'Content-Length': '496', 'X-GitHub-Request-Id': 'FDA0:086C:1A2C893:4404F95:5EEF07E6'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `content` and `text` typically have the response information that you are interested in. `text` is in Unicode format and `content` is in byte format. When the response is in text, you should use `.text` so that the encoding can be automatically inferred by Python. When the response is in byte (for example, if you send a GET request to an image link, the response will be binary data), you should use `content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"current_user_url\":\"https://api.github.com/user\",\"current_user_authorizations_html_url\":\"https://github.com/settings/connections/applications{/client_id}\",\"authorizations_url\":\"https://api.github.com/authorizations\",\"code_search_url\":\"https://api.github.com/search/code?q={query}{&page,per_page,sort,order}\",\"commit_search_url\":\"https://api.github.com/search/commits?q={query}{&page,per_page,sort,order}\",\"emails_url\":\"https://api.github.com/user/emails\",\"emojis_url\":\"https://api.github.com/emojis\",\"events_url\":\"https://api.github.com/events\",\"feeds_url\":\"https://api.github.com/feeds\",\"followers_url\":\"https://api.github.com/user/followers\",\"following_url\":\"https://api.github.com/user/following{/target}\",\"gists_url\":\"https://api.github.com/gists{/gist_id}\",\"hub_url\":\"https://api.github.com/hub\",\"issue_search_url\":\"https://api.github.com/search/issues?q={query}{&page,per_page,sort,order}\",\"issues_url\":\"https://api.github.com/issues\",\"keys_url\":\"https://api.github.com/user/keys\",\"label_search_url\":\"https://api.github.com/search/labels?q={query}&repository_id={repository_id}{&page,per_page}\",\"notifications_url\":\"https://api.github.com/notifications\",\"organization_url\":\"https://api.github.com/orgs/{org}\",\"organization_repositories_url\":\"https://api.github.com/orgs/{org}/repos{?type,page,per_page,sort}\",\"organization_teams_url\":\"https://api.github.com/orgs/{org}/teams\",\"public_gists_url\":\"https://api.github.com/gists/public\",\"rate_limit_url\":\"https://api.github.com/rate_limit\",\"repository_url\":\"https://api.github.com/repos/{owner}/{repo}\",\"repository_search_url\":\"https://api.github.com/search/repositories?q={query}{&page,per_page,sort,order}\",\"current_user_repositories_url\":\"https://api.github.com/user/repos{?type,page,per_page,sort}\",\"starred_url\":\"https://api.github.com/user/starred{/owner}{/repo}\",\"starred_gists_url\":\"https://api.github.com/gists/starred\",\"user_url\":\"https://api.github.com/users/{user}\",\"user_organizations_url\":\"https://api.github.com/user/orgs\",\"user_repositories_url\":\"https://api.github.com/users/{user}/repos{?type,page,per_page,sort}\",\"user_search_url\":\"https://api.github.com/search/users?q={query}{&page,per_page,sort,order}\"}'\n",
      "\n",
      "\n",
      "\n",
      "{\"current_user_url\":\"https://api.github.com/user\",\"current_user_authorizations_html_url\":\"https://github.com/settings/connections/applications{/client_id}\",\"authorizations_url\":\"https://api.github.com/authorizations\",\"code_search_url\":\"https://api.github.com/search/code?q={query}{&page,per_page,sort,order}\",\"commit_search_url\":\"https://api.github.com/search/commits?q={query}{&page,per_page,sort,order}\",\"emails_url\":\"https://api.github.com/user/emails\",\"emojis_url\":\"https://api.github.com/emojis\",\"events_url\":\"https://api.github.com/events\",\"feeds_url\":\"https://api.github.com/feeds\",\"followers_url\":\"https://api.github.com/user/followers\",\"following_url\":\"https://api.github.com/user/following{/target}\",\"gists_url\":\"https://api.github.com/gists{/gist_id}\",\"hub_url\":\"https://api.github.com/hub\",\"issue_search_url\":\"https://api.github.com/search/issues?q={query}{&page,per_page,sort,order}\",\"issues_url\":\"https://api.github.com/issues\",\"keys_url\":\"https://api.github.com/user/keys\",\"label_search_url\":\"https://api.github.com/search/labels?q={query}&repository_id={repository_id}{&page,per_page}\",\"notifications_url\":\"https://api.github.com/notifications\",\"organization_url\":\"https://api.github.com/orgs/{org}\",\"organization_repositories_url\":\"https://api.github.com/orgs/{org}/repos{?type,page,per_page,sort}\",\"organization_teams_url\":\"https://api.github.com/orgs/{org}/teams\",\"public_gists_url\":\"https://api.github.com/gists/public\",\"rate_limit_url\":\"https://api.github.com/rate_limit\",\"repository_url\":\"https://api.github.com/repos/{owner}/{repo}\",\"repository_search_url\":\"https://api.github.com/search/repositories?q={query}{&page,per_page,sort,order}\",\"current_user_repositories_url\":\"https://api.github.com/user/repos{?type,page,per_page,sort}\",\"starred_url\":\"https://api.github.com/user/starred{/owner}{/repo}\",\"starred_gists_url\":\"https://api.github.com/gists/starred\",\"user_url\":\"https://api.github.com/users/{user}\",\"user_organizations_url\":\"https://api.github.com/user/orgs\",\"user_repositories_url\":\"https://api.github.com/users/{user}/repos{?type,page,per_page,sort}\",\"user_search_url\":\"https://api.github.com/search/users?q={query}{&page,per_page,sort,order}\"}\n"
     ]
    }
   ],
   "source": [
    "print(response.content)\n",
    "print(\"\\n\\n\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the above response is not particularly easy to read, because it is in JSON (JavaScript Object Notation) format. Fortunately, JSON integrates with Python well, and you can seamlessly convert from a JSON string to a Python dictionary or vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current_user_url': 'https://api.github.com/user',\n",
       " 'current_user_authorizations_html_url': 'https://github.com/settings/connections/applications{/client_id}',\n",
       " 'authorizations_url': 'https://api.github.com/authorizations',\n",
       " 'code_search_url': 'https://api.github.com/search/code?q={query}{&page,per_page,sort,order}',\n",
       " 'commit_search_url': 'https://api.github.com/search/commits?q={query}{&page,per_page,sort,order}',\n",
       " 'emails_url': 'https://api.github.com/user/emails',\n",
       " 'emojis_url': 'https://api.github.com/emojis',\n",
       " 'events_url': 'https://api.github.com/events',\n",
       " 'feeds_url': 'https://api.github.com/feeds',\n",
       " 'followers_url': 'https://api.github.com/user/followers',\n",
       " 'following_url': 'https://api.github.com/user/following{/target}',\n",
       " 'gists_url': 'https://api.github.com/gists{/gist_id}',\n",
       " 'hub_url': 'https://api.github.com/hub',\n",
       " 'issue_search_url': 'https://api.github.com/search/issues?q={query}{&page,per_page,sort,order}',\n",
       " 'issues_url': 'https://api.github.com/issues',\n",
       " 'keys_url': 'https://api.github.com/user/keys',\n",
       " 'label_search_url': 'https://api.github.com/search/labels?q={query}&repository_id={repository_id}{&page,per_page}',\n",
       " 'notifications_url': 'https://api.github.com/notifications',\n",
       " 'organization_url': 'https://api.github.com/orgs/{org}',\n",
       " 'organization_repositories_url': 'https://api.github.com/orgs/{org}/repos{?type,page,per_page,sort}',\n",
       " 'organization_teams_url': 'https://api.github.com/orgs/{org}/teams',\n",
       " 'public_gists_url': 'https://api.github.com/gists/public',\n",
       " 'rate_limit_url': 'https://api.github.com/rate_limit',\n",
       " 'repository_url': 'https://api.github.com/repos/{owner}/{repo}',\n",
       " 'repository_search_url': 'https://api.github.com/search/repositories?q={query}{&page,per_page,sort,order}',\n",
       " 'current_user_repositories_url': 'https://api.github.com/user/repos{?type,page,per_page,sort}',\n",
       " 'starred_url': 'https://api.github.com/user/starred{/owner}{/repo}',\n",
       " 'starred_gists_url': 'https://api.github.com/gists/starred',\n",
       " 'user_url': 'https://api.github.com/users/{user}',\n",
       " 'user_organizations_url': 'https://api.github.com/user/orgs',\n",
       " 'user_repositories_url': 'https://api.github.com/users/{user}/repos{?type,page,per_page,sort}',\n",
       " 'user_search_url': 'https://api.github.com/search/users?q={query}{&page,per_page,sort,order}'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can also call `response.json()` to get the above output, without having to import the `json` package. However, this package is also useful for [many functionalities](https://docs.python.org/3/library/json.html) other than displaying response content, so you are encouraged to import it every time you work with HTTP requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. HTTP request customization\n",
    "It is common that you may want to pass some parameters with your requests in order to customize the content you get back. For the `requests` library, we can specify a `params` parameter in the form of a dictionary. These parameters are later parsed down and added to the base url or the api-endpoint.\n",
    "\n",
    "How the keys and values are specified depends on the specific API; typically there will be an API documentation that provides detailed guidance. In this case, let's say we want to search for public Github repositories that use Python. We can consult the [Search API page](https://developer.github.com/v3/search/#search-repositories) and the [List of qualifiers](https://help.github.com/en/github/searching-for-information-on-github/searching-for-repositories) to compose a search query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://api.github.com/search/repositories\"\n",
    "      \n",
    "# defining a params dict for the parameters to be sent to the API\n",
    "PARAMS={'q': 'requests+language:python'}\n",
    "  \n",
    "# sending get request and saving the response as response object \n",
    "response = requests.get(url = URL, params = PARAMS)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The status code indicates a success -- let's see what the response content is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"total_count\":10503,\"incomplete_results\":false,\"items\":[{\"id\":4290214,\"node_id\":\"MDEwOlJlcG9zaXRvcnk0MjkwMjE0\",\"name\":\"grequests\",\"full_name\":\"spyoungtech/grequests\",\"private\":false,\"owner\":{\"login\":\"spyoungtech\",\"id\":15212758,\"node_id\":\"MDQ6VXNlcjE1MjEyNzU4\",\"avatar_url\":\"https://avatars2.githubusercontent.com/u/15212758?v=4\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/spyoungtech\",\"html_url\":\"https://github.com/spyoungtech\",\"followers_url\":\"https://api.github.com/users/spyoungtech/followers\",\"following_url\":\"https://api.github.com/users/spyoungtech/following{/other_user}\",\"gists_url\":\"https://api.github.com/users/spyoungtech/gists{/gist_id}\",\"starred_url\":\"https://api.github.com/users/spyoungtech/starred{/owner}{/repo}\",\"subscriptions_url\":\"https://api.github.com/users/spyoungtech/subscriptions\",\"organizations_url\":\"https://api.github.com/users/spyoungtech/orgs\",\"repos_url\":\"https://api.github.com/users/spyoungtech/repos\",\"events_url\":\"https://api.github.com/users/spyoungtech/events{/privacy}\",\"received_events_url\":\"https://api.github.com/users/spyoungtech/received_events\",\"type\":\"User\",\"site_admin\":false},\"html_url\":\"https://github.com/spyoungtech/grequests\",\"description\":\"Requests + Gevent = <3\",\"fork\":false,\"url\":\"https://api.github.com/repos/spyoungtech/grequests\",\"forks_url\":\"https://api.github.com/repos/spyoungtech/grequests/forks\",\"keys_url\":\"https://api.github.com/repos/spyoungtech/grequests/keys{/key_id}\",\"collaborators_url\":\"https://api.github.com/repos/spyoungtech/grequests/collaborators{/collaborator}\",\"teams_url\":\"https://api.github.com/repos/spyoungtech/grequests/teams\",\"hooks_url\":\"https://api.github.com/repos/spyoungtech/grequests/hooks\",\"issue_events_url\":\"https://api.github.com/repos/spyoungtech/grequests/issues/events{/number}\",\"events_url\":\"https://api.github.com/repos/spyoungtech/grequests/events\",\"assignees_url\":\"https://api.github.com/repos/spyoungtech/grequests/assignees{/user}\",\"branches_url\":\"https://api.github.com/repos/spyoung'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our response contains around 10000 results, where each result is a Github repository link that contains Python code. Let's check out the `html_url` of the first result to confirm:\n",
    "\n",
    "https://github.com/spyoungtech/grequests\n",
    "\n",
    "This is indeed a Python repository (and a popular one!), so we got what we wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the majority of cases, you would also need an authenticated API key to send requests to an API; this key can be obtained by manually signing up for a developer account. For example, to send a request to the Yelp API, you can request a key [here](https://www.yelp.com/developers/documentation/v3/authentication), save it in a file called `api_key.txt`, then append its content to the request header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"businesses\": [{\"id\": \"JLbgvGM4FXh9zNP4O5ZWjQ\", \"alias\": \"meat-and-potatoes-pittsburgh\", \"name\": \"Meat & Potatoes\", \"image_url\": \"https://s3-media4.fl.yelpcdn.com/bphoto/Rc_WMhLcgKSAJnsitlJj1g/o.jpg\", \"is_closed\": false, \"url\": \"https://www.yelp.com/biz/meat-and-potatoes-pittsburgh?adjust_creative=5x_un5xfjVKExwcT_HwPvA&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=5x_un5xfjVKExwcT_HwPvA\", \"review_count\": 2023, \"categories\": [{\"alias\": \"gastropubs\", \"title\": \"Gastropubs\"}], \"rating\": 4.0, \"coordinates\": {\"latitude\": 40.4431445862832, \"longitude\": -80.0011037907791}, \"transactions\": [\"delivery\", \"restaurant_reservation\"], \"price\": \"$$$\", \"location\": {\"address1\": \"649 Penn Ave\", \"address2\": \"\", \"address3\": \"\", \"city\": \"Pittsburgh\", \"zip_code\": \"15222\", \"country\": \"US\", \"state\": \"PA\", \"display_address\": [\"649 Penn Ave\", \"Pittsburgh, PA 15222\"]}, \"phone\": \"+14123257007\", \"display_phone\": \"(412) 325-7007\", \"distance\": 1870.1784637351072}, {\"id\": \"woXlprCuowrLJswWere3TQ\", \"alias\": \"t\\\\u00e4k\\\\u014d-pittsburgh\", \"name\": \"t\\\\u00e4k\\\\u014d\", \"image_url\": \"https://s3-media1.fl.yelpcdn.com/bphoto/W2J52omHmHj54VA4aZffZw/o.jpg\", \"is_closed\": false, \"url\": \"https://www.yelp.com/biz/t%C3%A4k%C5%8D-pittsburgh?adjust_creative=5x_un5xfjVKExwcT_HwPvA&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=5x_un5xfjVKExwcT_HwPvA\", \"review_count\": 1532, \"categories\": [{\"alias\": \"newamerican\", \"title\": \"American (New)\"}, {\"alias\": \"mexican\", \"title\": \"Mexican\"}], \"rating\": 4.5, \"coordinates\": {\"latitude\": 40.4422285652929, \"longitude\": -80.0019846968895}, \"transactions\": [\"restaurant_reservation\"], \"price\": \"$$\", \"location\": {\"address1\": \"214 6th St\", \"address2\": \"\", \"address3\": \"\", \"city\": \"Pittsburgh\", \"zip_code\": \"15222\", \"country\": \"US\", \"state\": \"PA\", \"display_address\": [\"214 6th St\", \"Pittsburgh, PA 15222\"]}, \"phone\": \"+14124718256\", \"display_phone\": \"(412) 471-8256\", \"distance\": 1955.9825424877417}, {\"id\": \"dLc1d1zwd1Teu2QED5TmlA\", \"alias\": \"nood'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key = open(\"api_key.txt\").read().strip()\n",
    "headers = headers = {'Authorization': 'Bearer ' + api_key}\n",
    "params = {\"location\" : \"Pittsburgh\"}\n",
    "response = requests.get(\"https://api.yelp.com/v3/businesses/search\", params = params, headers = headers)\n",
    "print(response.status_code)\n",
    "response.text[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have searched for all restaurants in Pittsburgh, and the status code 200 indicates a successful query. The returned JSON maps the key `business` to a list of JSONs, one for each restaurant. Let's see how many there are in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses = json.loads(response.text)['businesses']\n",
    "len(businesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly we only get 20 results, but there should be more than 20 restaurants in Pittsburgh! This is due to *pagination*, which is a safeguard against returning too much data in a single request, which imposes heavy burden on the server. Instead, the API would divide the response results into several pages, and return one page per request. To get all the results we want, we would then need to send multiple requests, each with a different \"page number\" parameter.\n",
    "\n",
    "Let's start by recording the 20 business ids that we got from the last request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JLbgvGM4FXh9zNP4O5ZWjQ', 'woXlprCuowrLJswWere3TQ', 'dLc1d1zwd1Teu2QED5TmlA', 'SmkYLXEYhzwUZdS6TAevHg', 'j83FLfnjvSHw-vrCNHuTCg', 'NoF90rswXBHESSyDaWeKKA', 'MKYcOZSpMwJK7uwacK13EA', 'd2ZQRjuizstCTnicysmpMQ', 'BcLFIr4wtd3GQ3fnz15yDQ', 'S8urdN6ACnRQUm-7o9An8w', 'VgWvHMs5TJ2lr0ec2DaMtQ', 'sc__kdcFV4IcNTfBx1707w', 'xcmmTXhuMx2fZF2Bt69F4w', 'RKVYQ00LvK0_FO6Ll7lvOg', 'hcFSc0OHgZJybnjQBrL_8Q', 'Ul6JwluSTm12PVDIqnNaTg', 'LQFmktF43j2NPncKdNd9mg', '4mYS-4UOjTKgsf0tX1_IkQ', 'Cf0iV72DTqR0ggBje2d0sg', '0PCBt3JKD6IooicImKNBzA']\n"
     ]
    }
   ],
   "source": [
    "print([b['id'] for b in businesses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we send another query to request the next \"page\" of businesses. For the Yelp API, this can be done by the `offset` parameter, which specifies the starting index of our restaurant list. Since we already got the restaurants from 0 to 19, we can now set `offset` to 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sMzNLdhJZGzYirIWt-fMAg', 'gldPX9ANF5Nic0N7igu2og', 'wmCBxE0PfLZD8sxIwAY59Q', 'Z2NIEKTVVP2nEmOF4jNxqQ', 'eIedjt0mHVKDFmfhtfKSAQ', 'lKom12WnYEjH5FFemK3M1Q', 'VxRlBe2wjtycFWSZm1orTA', 'LsjYGLrWe6psx1y5m2J-4A', 'PU-CSnMYXizOS3uhr316eg', 'dQj5DLZjeDK3KFysh1SYOQ', 'SvCjBtbN1cKElDKPTw9dOA', '0bjFYstj8USMzEV4ZQldjA', 'efSbCWuU0FJbLmPC5CDfdg', 'bXCWON2Me0o86qvAb-XZPQ', 'oeW0vIYd3rUnAPgmD4fEFg', 'ejaUQ1hYo7Q7xCL1HdPINw', '7z2x16M7IuG8KPfMsyVrKA', 'Fozo0B-y42EhRMomR0K5vQ', 'BskUTTscZ1XGa9ev7TlfeQ', 'dRbmeC5hl211hH-WeMtv-g']\n"
     ]
    }
   ],
   "source": [
    "params[\"offset\"] = 20\n",
    "response = requests.get(\"https://api.yelp.com/v3/businesses/search\", params = params, headers = headers)\n",
    "print([b['id'] for b in json.loads(response.text)[\"businesses\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we indeed have 20 different ids. In general you typically want to collect a large number of restaurants (say 10000), and you can use a for-loop to send multiple requests. However, make sure to keep in mind the *rate limit*: you should pause slightly (at least 0.2 seconds) between consecutive reqeusts so as to not overwhelm the API and get blocked. The rate limit is also API-dependent; for example, with [Github API](https://developer.github.com/v3/search/#search-repositories), you can send up to 30 authenticated requests or 10 unauthenticated requests per minute (so you would need a longer timeout than 0.2 seconds between requests)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Web scraping with BeautifulSoup\n",
    "While API provides a convenient way to collect data, not all websites have a dedicated API set up for developers (in fact, most do not). In many cases, the only data available to us is the source code of the same HTML page that we would see through the web browser. The good news is that this HTML content may still contain the data we need; the bad news is that we now have to parse it ourselves. Fortunately, the `BeautifulSoup` library in Python makes this task a lot easier.\n",
    "\n",
    "Before we start, if you aren't familiar with HTML and CSS, here is a [CSS selector cheatsheet](https://gist.github.com/magicznyleszek/809a69dd05e1d5f12d01) that may come in handy when you go through the materials below and Project 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. BeautifulSoup basics\n",
    "To begin parsing, we can input the HTML string of a page (obtained through a GET request) to a BeautifulSoup object. We can also specify the parser for this string; the recommended option is `html.parser`. For now let's create our own simple HTML source code to play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<div class=\"container\" id=\"title\">\n",
    "    <div class=\"row\" hello_attr=\"hello\">\n",
    "        <p onclick=\"execute();\">\n",
    "            <img src=\"pic.png\"/>\n",
    "            Click me\n",
    "            <br />\n",
    "        </p>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping an HTML page typically involves identifying the traversal path through the page structure to what you want, and then providing that path to BeautifulSoup using HTML tags and CSS selectors. There are three common methods you can use:\n",
    "\n",
    "* `find` returns the first HTML tag which is a descendant of the caller and satisfies the search condition, or `None` if no match is found. Typically you would provide the tag name and any custom attribute that your targeted tag has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p onclick=\"execute();\">\n",
       "<img src=\"pic.png\"/>\n",
       "            Click me\n",
       "            <br/>\n",
       "</p>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look for the first p element\n",
    "soup.find(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"container\" id=\"title\">\n",
       "<div class=\"row\" hello_attr=\"hello\">\n",
       "<p onclick=\"execute();\">\n",
       "<img src=\"pic.png\"/>\n",
       "            Click me\n",
       "            <br/>\n",
       "</p>\n",
       "</div>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look for the first div element whose id is \"title\"\n",
    "soup.find(\"div\", id = \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"row\" hello_attr=\"hello\">\n",
       "<p onclick=\"execute();\">\n",
       "<img src=\"pic.png\"/>\n",
       "            Click me\n",
       "            <br/>\n",
       "</p>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look for the first div element whose custom attribute \"hello_attr\" has value \"hello\"\n",
    "soup.find(\"div\", {\"hello_attr\" : \"hello\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we are using a dictionary to specify which attribute to look for and what its corresponding values is. For some common attributes such as `class` or `id` or `itemprop`, there are also built-in parameters to quickly specify them. For example, in the second code block above we could use `find(\"div\", id = \"title\")` instead of `find(\"div\", {\"id\" : \"title\"})`. Note that if you want to find by class name, the corresponding parameter is `class_`, not `class`, because the latter is a reserved Python keyword. In addition, if you want to search for a tag that has an id, but it doesn't matter with the id is, you can simply use `.find(id = True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `find_all` returns a list of HTML tags that are descendants of the caller and satisfy the search condition, or an empty list if no match is found. It accepts the same input parameters as `find`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"container\" id=\"title\">\n",
       " <div class=\"row\" hello_attr=\"hello\">\n",
       " <p onclick=\"execute();\">\n",
       " <img src=\"pic.png\"/>\n",
       "             Click me\n",
       "             <br/>\n",
       " </p>\n",
       " </div>\n",
       " </div>,\n",
       " <div class=\"row\" hello_attr=\"hello\">\n",
       " <p onclick=\"execute();\">\n",
       " <img src=\"pic.png\"/>\n",
       "             Click me\n",
       "             <br/>\n",
       " </p>\n",
       " </div>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all divs\n",
    "soup.find_all(\"div\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all p tags whose class is \"hello\"\n",
    "# empty list because no match is found\n",
    "soup.find_all(\"p\", class_ = \"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `select` functions similarly as `find_all` in that it returns a list of matches, or an empty list if no match is found. The difference is that it only takes as input a CSS selector string. If you are familiar with CSS, `.search` is a good option to concisely specify exactly what you need. Note that BeautifulSoup doesn't have support for many CSS selectors, such as chaining attributes `a[href][src]`; however, what it does support should be sufficient for many web scraping tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p onclick=\"execute();\">\n",
       " <img src=\"pic.png\"/>\n",
       "             Click me\n",
       "             <br/>\n",
       " </p>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all div tags whose id is \"title\"\n",
    "# for each div tag from the last step, find all div tags whose class is \"row\"\n",
    "# for each div tag from the last step, find all p tags\n",
    "soup.select(\"div#title div.row p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the same task using `find_all`, we would need to chain multiple calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p onclick=\"execute();\">\n",
       " <img src=\"pic.png\"/>\n",
       "             Click me\n",
       "             <br/>\n",
       " </p>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    z\n",
    "    for x in soup.find_all(\"div\", id = \"title\")\n",
    "    for y in soup.find_all(\"div\", class_ = \"row\")\n",
    "    for z in soup.find_all(\"p\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in this specific case, we can use `.find` since there are no duplicate instances of any tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p onclick=\"execute();\">\n",
       "<img src=\"pic.png\"/>\n",
       "            Click me\n",
       "            <br/>\n",
       "</p>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"div\", id = \"title\").find(\"div\", class_ = \"row\").find(\"p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be wondering why we need to specify three levels of HTML tags while doing a `soup.find(\"p\")` is enough. In this case it actually is enough, but in the more general case, a website can contain many `p` tags at different locations -- in order to avoid capturing tags you don't intend to, you would need to be as specific about your input path as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the targeted element / list of elements, we can call some methods to extract their content. The most common method is `get_text()`, which gives the inner text content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n            Click me\\n            \\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"p\").get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now a normal Python string that you can process, for example by calling `.strip()` to remove leading and trailing whitespaces. To get an element attribute, you can also use indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'execute();'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"p\")[\"onclick\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should also check out the [official BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/). This is a self-contained tutorial with many useful examples and advanced scraping functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. BeautifulSoup example\n",
    "Now let's do an example where we want to collect the names of all faculties in the MCDS program. We first retrieve the HTML code of the [faculty page](https://mcds.cs.cmu.edu/directory/all/154/1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<!--[if IEMobile 7]><html class=\"iem7\"  lang=\"en\" dir=\"ltr\"><![endif]-->\n",
      "<!--[if lte IE 6]><html class=\"lt-ie9 lt-ie8 lt-ie7\"  lang=\"en\" dir=\"ltr\"><![endif]-->\n",
      "<!--[if (IE 7)&(!IEMobile)]><html class=\"lt-ie9 lt-ie8\"  lang=\"en\" dir=\"ltr\"><![endif]-->\n",
      "<!--[if IE 8]><html class=\"lt-ie9\"  lang=\"en\" dir=\"ltr\"><![endif]-->\n",
      "<!--[if (gte IE 9)|(gt IEMobile 7)]><!-->\n",
      "<html dir=\"ltr\" lang=\"en\">\n",
      " <!--<![endif]-->\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <link href=\"https://mcds.cs.cmu.edu/sites/all/themes/mcds2015/favicon.ico\" rel=\"shortcut icon\" type=\"image/vnd.microsoft.icon\"/>\n",
      "  <meta content=\"Drupal 7 (http://drupal.org)\" name=\"Generator\"/>\n",
      "  <title>\n",
      "   Master of Computational Data Science - Faculty | Carnegie Mellon University - Language Technologies Institute\n",
      "  </title>\n",
      "  <meta content=\"width\" name=\"MobileOptimized\"/>\n",
      "  <meta content=\"true\" name=\"HandheldFriendly\"/>\n",
      "  <meta content=\"width=device-width\" name=\"viewport\"/>\n",
      "  <!--[if IEMobile]><meta http-equiv=\"cleartype\" content=\"on\"><![endif]-->\n",
      "  <style>\n",
      "   @import url(\"https://mcds.cs.cmu.edu/modules/system/system.base.css?pyful9\");\n",
      "  </style>\n",
      "  <style>\n",
      "   @import url(\"https://mcds.cs.cmu.edu/sites/all/modules/scroll_to_top/scroll_to_top.css?pyful9\");\n",
      "@import url(\"https://mcds.cs.cmu.edu/sites/all/modules/calendar/css/calendar_multiday.css?pyful9\");\n",
      "@import url(\"https://mcds.cs.cmu.edu/sites/all/modules/date/date_api/date.css?pyful9\");\n",
      "@import url(\"https://mcds.cs.cmu.edu/sites/all/modules/date/date_popup/themes/datepicker.1.7.css?pyful9\");\n",
      "@import url(\"https://mcds.cs.cmu.edu/sites/all/modules/date/date_repeat_field/date_repeat_field.css?pyful9\");\n",
      "@import url(\"https://mcds.cs.cmu.edu/modules/field/theme/field.css?pyful9\");\n",
      "@import url(\"https://mcds.cs.cmu.edu/sites/all/modules/fitvids/fitvids.css?pyful9\");\n",
      "@import url(\"https://mcds.cs.cmu.edu/sites/all/modules/menu_attach_block/menu_attach_block.css?pyful9\");\n",
      "@import url(\"https://mcds.cs.cmu.edu/modules/node/node.css?pyful9\");\n",
      "@import url(\"https://mcds.cs.cmu\n"
     ]
    }
   ],
   "source": [
    "url = \"https://mcds.cs.cmu.edu/directory/all/154/1\"\n",
    "soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "print(soup.prettify()[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open the same page on a web browser, so that we can investigate the HTML structure. We can right click on the name of the first faculty on the list, George Amvrosidias, and select \"Inspect\" to check which HTML tag contains that name.\n",
    "\n",
    "![mcds_faculty_1](http://clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-data-collection-extraction-primer/mcds_faculty_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our target tag is an `<h2>`. Let's start with finding all `h2` tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2 class=\"element-invisible\">Search form</h2>,\n",
       " <h2>George Amvrosidias</h2>,\n",
       " <h2>Jeffrey Bigham</h2>,\n",
       " <h2>Jamie Callan</h2>,\n",
       " <h2>William Cohen</h2>,\n",
       " <h2>Lorrie Cranor</h2>,\n",
       " <h2>Christos Faloutsos</h2>,\n",
       " <h2>Kayvon Fatahalian</h2>,\n",
       " <h2>Gregory Ganger</h2>,\n",
       " <h2>Matthias Grabmair</h2>,\n",
       " <h2>Geoff Kauffman</h2>,\n",
       " <h2>Aniket Kittur</h2>,\n",
       " <h2>Robert Kraut</h2>,\n",
       " <h2>Jennifer Lucas</h2>,\n",
       " <h2>Todd Mowry</h2>,\n",
       " <h2>Daniel Neill</h2>,\n",
       " <h2>Eric Nyberg</h2>,\n",
       " <h2>Andrew Pavlo</h2>,\n",
       " <h2>Ronald Rosenfeld</h2>,\n",
       " <h2>Alexander Rudnicky</h2>,\n",
       " <h2>Majd Sakr</h2>,\n",
       " <h2 class=\"element-invisible\">Pages</h2>,\n",
       " <h2>Contact Us</h2>,\n",
       " <h2>Connect</h2>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"h2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this does give us all the faculty names, but also some unneeded content like \"Search form\" and \"Contact Us\". We want to avoid having to remove these manually, so let's form a stricter search condition -- this time we search for all the `h2` tags inside an `a` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2>George Amvrosidias</h2>,\n",
       " <h2>Jeffrey Bigham</h2>,\n",
       " <h2>Jamie Callan</h2>,\n",
       " <h2>William Cohen</h2>,\n",
       " <h2>Lorrie Cranor</h2>,\n",
       " <h2>Christos Faloutsos</h2>,\n",
       " <h2>Kayvon Fatahalian</h2>,\n",
       " <h2>Gregory Ganger</h2>,\n",
       " <h2>Matthias Grabmair</h2>,\n",
       " <h2>Geoff Kauffman</h2>,\n",
       " <h2>Aniket Kittur</h2>,\n",
       " <h2>Robert Kraut</h2>,\n",
       " <h2>Jennifer Lucas</h2>,\n",
       " <h2>Todd Mowry</h2>,\n",
       " <h2>Daniel Neill</h2>,\n",
       " <h2>Eric Nyberg</h2>,\n",
       " <h2>Andrew Pavlo</h2>,\n",
       " <h2>Ronald Rosenfeld</h2>,\n",
       " <h2>Alexander Rudnicky</h2>,\n",
       " <h2>Majd Sakr</h2>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select(\"a h2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this works! In case it still doesn't, we can narrow down the scope even more, e.g., \n",
    "\n",
    "`select(\"span.field-content a h2\")`\n",
    "\n",
    "until we get exactly what is needed. As the last step, let's now extract the string names and put them in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['George Amvrosidias', 'Jeffrey Bigham', 'Jamie Callan', 'William Cohen', 'Lorrie Cranor', 'Christos Faloutsos', 'Kayvon Fatahalian', 'Gregory Ganger', 'Matthias Grabmair', 'Geoff Kauffman', 'Aniket Kittur', 'Robert Kraut', 'Jennifer Lucas', 'Todd Mowry', 'Daniel Neill', 'Eric Nyberg', 'Andrew Pavlo', 'Ronald Rosenfeld', 'Alexander Rudnicky', 'Majd Sakr']\n"
     ]
    }
   ],
   "source": [
    "print([name.get_text() for name in soup.select(\"a h2\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above list is not complete, since the faculty directory actually spans 2 pages. You can extract the URL to the second page using BeautifulSoup, send another GET request to that second page, and repeat the above process to fill in the remaining faculties.\n",
    "\n",
    "As another note, if you look at the top portion of the HTML code in the screenshot above, you will see a `div` tag with a very long class name `view-dom-id-55a0ca...` If this looks like a computer-generated name to you, the reason is that it is in fact computer-generated, and you may see a different name if you access the same webpage at a different time. Therefore, make sure you do not hard code these names in your BeautifulSoup searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Navigating dynamic web pages with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Introduction\n",
    "BeautifulSoup does a good job when you have all of the content needed in the HTML page source, i.e., when you request a *static* webpage. However, thanks to advances in JavaScript (😢) many modern webpages are *dynamic* -- their content will change based on user interaction. In other words, you may need to perform some action on the webpage itself, such as clicking a button or entering some text.\n",
    "\n",
    "As an example, consider the following search page: https://www.foxnews.com/search-results/search?q=president\n",
    "We see that the search results are ordered by their recency. Let's say we want to search for all articles in 03/2020. We can first try doing it manually to see what the page interaction looks like, by clicking on the \"By Content\" and \"Date Range\" boxes:\n",
    "![fox_news](http://clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-data-collection-extraction-primer/fox_news.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When clicking on Search, we see that the search results are updated to indeed show articles in March; however, the page URL doesn't change at all. In other words, there are no GET parameters for content type or date range, so in order to filter our result by these attributes, we need to access the page and do the button clicking ourselves, which is not scalable.\n",
    "\n",
    "This is when Selenium comes in handy. Selenium is an open source automated testing suite for web application testing across different platforms. The basic idea is that if you are developing a website and you expect an element X to show up when the user clicks on button Y, you can use Selenium to simulate the Y button clicking and to then check the presence of X. Note that it was not built for web scraping, but its functionality is exactly what we want in this case, so let's go ahead and get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Installation\n",
    "After installing the `selenium` Python package, the next steps depend on whether you are running Jupyter locally or on an online environment, i.e., Google Colab. Note that Selenium **does not work on Azure Notebook**, since we do not have permission to install packages via `apt-get` in the Azure terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. For local Jupyter notebook\n",
    "Other than the `selenium` package itself, you will also need a web browser and the corresponding webdriver. The browsers that work best with Selenium are either some version of Chrome (Google Chrome or Chromium) or Firefox.\n",
    "\n",
    "If you use Chrome, head to [this page](https://chromedriver.chromium.org/downloads) and download the appropriate webdriver zip file, based on your operating system and Chrome version. Then unzip it in the **same directory as your Jupyter notebook**; if you are on Mac OS or Linux, run the following Bash command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Firefox, head to [this page](https://github.com/mozilla/geckodriver/releases) and download the latest driver version (currently 0.26.0), based on your operating system. Then unzip it in the **same directory as your Jupyter notebook**; if you are on Mac OS or Linux, run the following Bash command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./geckodriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 For Google Colab\n",
    "If you use Colab, there is no need to download anything. Simply create this Bash cell at the beginning of the notebook and run it. If you use this in your project 3 notebook, remember to tag this cell with `excluded_from_script` so that it doesn't get run by the autograder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!apt-get update \n",
    "!apt-get install chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Selenium functionalities\n",
    "From now we will assume that you have either the `chromedriver` or `geckodriver` executable in the same folder as your notebook. We will import the `webdriver` from `selenium` and initialize the two drivers as follows. Note that if you use Windows, you may have to append `.exe` to the `executable_path` parameter here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "def init_chromedriver(debug = False):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if not debug:\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument(\"--disable-setuid-sandbox\")\n",
    "    return webdriver.Chrome(executable_path = \"./chromedriver\", options = options)\n",
    "\n",
    "\n",
    "def init_geckodriver(debug = False):\n",
    "    options = webdriver.FirefoxOptions()\n",
    "    if not debug:\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument(\"--disable-setuid-sandbox\")\n",
    "    return webdriver.Firefox(executable_path = \"./geckodriver\", options = options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Chrome driver for the rest of this primer; if you prefer Firefox, simply edit the following cell to call `init_geckodriver` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = init_chromedriver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you get an \"executable must be in path\" error from the above cell, again make sure that your driver executable is in the same directory as this `data_collection_extraction_primer.ipynb` file. You can run the following cell and check that the driver name shows up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GiftOfTheMagi.pdf                       fox_news_content_type.png\r\n",
      "api_key.txt                             fox_news_search_results.png\r\n",
      "\u001b[31mchromedriver\u001b[m\u001b[m                            \u001b[31mgeckodriver\u001b[m\u001b[m\r\n",
      "data_collection_extraction_primer.ipynb mcds_faculty_1.png\r\n",
      "fox_news.png                            text_data_processing_primer.ipynb\r\n",
      "fox_news_articles_selected.png\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the functionalities of the Python selenium webdriver are described in the [Selenium documentation](https://selenium-python.readthedocs.io/getting-started.html). In our context we are mostly interested in section 3 (Navigating), 4 (Locating Elements), and 5 (Waits). In particular, here are the commonly used methods:\n",
    "\n",
    "* **Page navigation**: `.get` navigates the webdriver to a given webpage, based on the input string URL.\n",
    "* **Element search**: `.find_element_by_*` or `.find_elements_by_*` are roughly the equivalence of the search functionalities in BeautifulSoup. Typically we only need to use `find_elements_by_css_selector` since it is the most flexible.\n",
    "* **Element action**: once you get the returned object from one of the element search functions, you can call a number of methods to simulate actions on that object. Some common actions include `click`, `double_click`, and `send_keys`. See Section 7.2 of the [API](https://selenium-python.readthedocs.io/api.html) for their usage. \n",
    "\n",
    "Once we have performed the necessary (simulated) user interactions, we can call `driver.page_source` to get the string HTML of the current stage of the page, and pass this to BeautifulSoup for scraping as we did before.\n",
    "\n",
    "Let's walk through an example of getting the first 10 articles about the term \"president\" in March from Fox News. We begin by directing the webdriver to the search URL above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.foxnews.com/search-results/search?q=president\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first inspect the \"By Content\" box and see how we can select the \"Article\" from the dropdown menu. We will record the actions we took manually:\n",
    "\n",
    "1. Click on the \"Select Content Type\" box.\n",
    "1. Click on the \"Article\" cell in the dropdown menu.\n",
    "\n",
    "After these two actions, we get the resulting HTML structure as follows:\n",
    "\n",
    "![content_type](http://clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-data-collection-extraction-primer/fox_news_content_type.png)\n",
    "\n",
    "Now we translate the above actions into Selenium function calls and specify one possible traversal path to the \"Action\" cell as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.foxnews.com/search-results/search?q=president\")\n",
    "\n",
    "content = driver.find_element_by_css_selector(\"div.filter.content\")\n",
    "content.click()\n",
    "content.find_element_by_css_selector(\"ul.option>li>label>input[value=\\\"%s\\\"]\"% \"Article\").click()\n",
    "content.click()\n",
    "\n",
    "html_content = driver.page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we check whether this works? We notice that in the browser, after we manually seleced \"Article\", there is a purple \"Article\" cell that pops up below the search query box:\n",
    "\n",
    "![article_selected](http://clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-data-collection-extraction-primer/fox_news_articles_selected.png)\n",
    "\n",
    "Since we did record the html content of the page after our Selenium calls, we can pass it to BeautifulSoup and check that this \"Article\" cell is present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li class=\"Article\" id=\"Article\">Article<span></span></li>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "soup.find(\"div\", class_ = \"search-terms\").find(\"li\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we did see the `<li>` Article cell from the screenshot, so we are good to go! Similarly, we can click on the other boxes and then the Search button to submit our query. Note that we will run `driver.get` again, as all Selenium codes should be excecuted together in the same scope; if you run some codes in one cell and some in another a moment later, you may get an \"element not interactable\" exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def autoclick(driver, min_month_val, min_day_val, max_month_val, max_day_val, year):\n",
    "    driver.get(\"https://www.foxnews.com/search-results/search?q=president\")\n",
    "\n",
    "    content = driver.find_element_by_css_selector(\"div.filter.content\")\n",
    "    content.click()\n",
    "    content.find_element_by_css_selector(\"ul.option>li>label>input[value=\\\"%s\\\"]\"% \"Article\").click()\n",
    "    content.click()\n",
    "\n",
    "    min_month = driver.find_element_by_css_selector(\"div.date.min div.sub.month\")\n",
    "    min_month.click()\n",
    "    min_month.find_element_by_css_selector(\"ul.option>li[id=\\\"%s\\\"]\"%min_month_val).click()\n",
    "\n",
    "    min_day = driver.find_element_by_css_selector(\"div.date.min div.sub.day\")\n",
    "    min_day.click()\n",
    "    min_day.find_element_by_css_selector(\"ul.option>li[id=\\\"%s\\\"]\"%min_day_val).click()\n",
    "\n",
    "    min_year = driver.find_element_by_css_selector(\"div.date.min div.sub.year\")\n",
    "    min_year.click()\n",
    "    min_year.find_element_by_css_selector(\"ul.option>li[id=\\\"%s\\\"]\"%year).click()\n",
    "\n",
    "    max_month = driver.find_element_by_css_selector(\"div.date.max div.sub.month\")\n",
    "    max_month.click()\n",
    "    max_month.find_element_by_css_selector(\"ul.option>li[id=\\\"%s\\\"]\"%max_month_val).click()\n",
    "\n",
    "    max_day = driver.find_element_by_css_selector(\"div.date.max div.sub.day\")\n",
    "    max_day.click()\n",
    "    max_day.find_element_by_css_selector(\"ul.option>li[id=\\\"%s\\\"]\"%max_day_val).click()\n",
    "\n",
    "    max_year = driver.find_element_by_css_selector(\"div.date.max div.sub.year\")\n",
    "    max_year.click()\n",
    "    max_year.find_element_by_css_selector(\"ul.option>li[id=\\\"%s\\\"]\"%year).click()\n",
    "\n",
    "    search = driver.find_element_by_css_selector(\"div.search-form a\")\n",
    "    search.click()\n",
    "\n",
    "    # wait a bit for the new search results to load\n",
    "    time.sleep(3)\n",
    "    return driver.page_source\n",
    "\n",
    "html_content = autoclick(driver, \"03\", \"01\", \"03\", \"31\", \"2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the resulting search result page looks like. We see that every result article is represented by an `<article>` tag, with some inner tags for the content.\n",
    "\n",
    "![fox_news_search_results](http://clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-data-collection-extraction-primer/fox_news_search_results.png)\n",
    "\n",
    "Let's first print out the published time of all articles in the result page, and make sure that they are in March (i.e., assuming we are in 06/2020, the time value should be \"3 months ago\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 months ago ', '3 months ago ', '3 months ago ', '3 months ago ', '3 months ago ', '3 months ago ', '3 months ago ', '3 months ago ', '3 months ago ', '3 months ago ']\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "articles = soup.find_all(\"article\")\n",
    "print([article.find(\"span\", class_ = \"time\").get_text() for article in articles])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was indeed the case! Now let's check that the article titles are also the same as what we see from the screenshot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trump calls himself a ‘wartime president’ over coronavirus as he invokes Defense Production Act',\n",
       " 'President Trump reveals he took coronavirus test',\n",
       " 'Vice President Pence, wife test negative for COVID-19',\n",
       " \"Brazil President Bolsonaro's son claims father tested negative for coronavirus despite earlier reports\",\n",
       " \"Former White House photographer dubs Andrew Cuomo 'acting president' amid COVID-19 outbreak\",\n",
       " 'Venezuela President Maduro wanted by DOJ for drug trafficking, Barr announces',\n",
       " 'Fox News hosting virtual coronavirus town hall with President Trump, White House task force',\n",
       " 'Trump downplays coronavirus threat, notes ‘common flu’ kills thousands every year',\n",
       " 'LIVE BLOG: Fox News hosts virtual coronavirus town hall with President Trump',\n",
       " 'Mexico suspends large gatherings over coronavirus days after its president urges residents to dine out']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[article.select(\"h2.title a\")[0].get_text() for article in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These check out as well, so we are good to go! Note that Fox News sometimes gives inconsistent search results even with the same search query, so if you rerun this code, your output may not look like the screenshots above, but it should be consistent with what you see on your browser at the time of running.\n",
    "\n",
    "As a bonus, in the following cell we put together a copy of all the Selenium codes above, but with `debug = True` in the driver initialization. When you run this cell, there will be an actual browser window that pops up and you can see Selenium performing the actions in real time; it's quite cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished autoclicking!\n"
     ]
    }
   ],
   "source": [
    "driver = init_chromedriver(debug = True)\n",
    "html_content = autoclick(driver, \"03\", \"01\", \"03\", \"31\", \"2020\")\n",
    "print(\"Finished autoclicking!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, the next steps are:\n",
    "1. Instead of extracting the article titles, you can extract their URLs, then visit each individual article page to parse their content.\n",
    "1. Note that we only get 10 articles because the rest are hidden; there is a \"Load More\" button at the end of the page that we have to click on to get more search results. This step can also be done with Selenium.\n",
    "\n",
    "In Project 3, you will get to practice with these two steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Dealing with cookies\n",
    "Certain sites such as https://nytimes.com/ or http://towardsdatascience.com/ may require you to sign in after you have accessed the site for a number of times. While this does not affect the Selenium operations if you use headless browser (by setting `debug = False`), it may prevent you from visiting the site manually to explore the page's HTML structure. Typically your visit history to a site is stored in local cookies, so if you simply clear the cookies for that site, you will be able to access it again.\n",
    "\n",
    "For example, You may see this popup when visiting an article on https://nytimes.com/:\n",
    "\n",
    "![nytimes_blocked](http://clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-data-collection-extraction-primer/nytimes_blocked.png)\n",
    "\n",
    "On Google Chrome you can reset cookies with the following steps:\n",
    "\n",
    "1. Go to the Cookies page [chrome://settings/siteData?search=cookies](chrome://settings/siteData?search=cookies)\n",
    "1. In the \"Search cookies\" box, type in \"nytimes\"\n",
    "1. You will see some cookies associated with `nytimes.com` listed. Click on \"Remove All Shown\" to remove all of them.\n",
    "1. On a new browser tab, reopen the NYT article you previously tried to access.\n",
    "\n",
    "For other browsers, you can also search for specific instructions on clearing cookies. For example, [this guide](https://support.mozilla.org/en-US/kb/clear-cookies-and-site-data-firefox) is for Firefox. You will perform webscraping on nytimes.com in Project 3, so keep this trick in mind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Some closing notes on Selenium\n",
    "While we have seen the power of Selenium in handling dynamic website interaction, it should be noted that Selenium was designed as a tool to test *your* own website, not to scrape *others*' websites. Many websites are not happy with you crawling their content in this manner, and they would either implement a captcha test to thwart the bots (in this case, your Selenium script) or downright block your IP. Therefore, use this tool carefully and responsibly. See [this article](https://levelup.gitconnected.com/web-scraping-with-selenium-in-python-8fde2f0fd559) for more details.\n",
    "\n",
    "The websites that you will scrape in Project 3 are not too strict about this; just make sure to pause shortly between consecutive requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parsing pdf files\n",
    "We have spent much of this primer covering how to parse online content from APIs or websites. There is actually an equally rich data source: pdf files. PDF stands for \"portable document format\" and is typically used for distribution of read-only files. Nowadays it is perhaps the most ubiquitous type of file that can be shared and read across different platforms.\n",
    "\n",
    "However, this popularity comes with a cost: because so many file formats can be converted to pdf (e.g., Word documents, Excel spreadsheets, Jupyter notebooks), a pdf file is much closer to an image than a (structured) document, despite having the term \"document\" in its name. Libraries that support parsing pdf files typically convert them to binary first, and then try to convert the binary back to text. In this section we will introduce one such library, called `pdfminer`. You can install it by running\n",
    "\n",
    "`pip3 install pdfminer.six`\n",
    "\n",
    "**Note**: do not run `pip3 install pdfminer`, since it will give you an [outdated version](https://github.com/euske/pdfminer). Instead, make sure to install `pdfminer.six` instead; after this installation we can still `import pdfminer` as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most straightforward way to extract text from a pdf file is by calling the `extract_text` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T h e   G i\\n\\nf\\n\\nt\\n\\n  o f\\n\\n \\n\\nt h e   M a g i\\n\\np\\n\\nT h e   G i f t   o f   t h e   M a g i\\n\\nThat was all. She had put it aside, one cent and then another and then \\n\\nONE DOLLAR AND EIGHTY-SEVEN CENTS.     \\n\\nanother,  in  her  careful  buying  of  meat  and  other  food.  Della  counted  \\n\\nit  three  times.  One  dollar  and  eighty-seven  cents.  And  the  next  day  \\n\\nwould be Christmas.\\n\\nThere was nothing to do but fall on the bed and cry. So Della did it. \\n\\nWhile  the  lady  of  the  home  is  slowly  growing  quieter,  we  can  \\n\\nlook at the home. Furnished rooms at a cost of $8 a week. There is lit-\\n\\ntle more to say about it.\\n\\nIn the hall below was a letter-box too small to hold a letter. There \\n\\nwas  an  electric  bell,  but  it  could  not  make  a  sound.  Also  there  was  a  \\n\\nname beside the door: “Mr. James Dillingham Young.”\\n\\n1\\n\\n\\x0cO .\\n\\n  H e n r y\\n\\nWhen the name was placed there, Mr. James Dillingham Young \\nwas being paid $30 a week. Now, when he was being paid only $20 a \\nweek, the name seemed too long and important. It should perhaps have \\nbeen “Mr. James D. Young.” But when Mr. James Dillingham Young \\nentered the furnished rooms, his name became very short indeed. Mrs. \\nJames  Dillingham  Young  put  her  arms  warmly  about  him  and  called  \\nhim “Jim.” You have already met her. She is Della.\\n\\nDella finished her crying and cleaned the marks of it from her face. \\nShe stood by the window and looked out with no interest. Tomorrow \\nwould be Christmas Day, and she had only $1.87 with which to buy  \\nJim a gift. She had put aside as much as she could for months, with this \\nresult. Twenty dollars a week is not much. Everything had cost more \\nthan she had expected. It always happened like that.\\n\\nOnly $ 1.87 to buy a gift for Jim. Her Jim. She had had many happy \\nhours planning something nice for him. Something nearly good enough. \\nSomething almost worth the honor of belonging to Jim.\\n\\nThere was a looking-glass between the windows of the room. Per-\\nhaps you have seen  the kind of looking-glass that  is placed  in  $8 fur-\\nnished  rooms.  It  was  very  narrow.  A  person  could  see  only  a  little  of \\nhimself at a time. However, if he was very thin and moved very quickly, \\nhe might be able to get a good view of himself. Della, being quite thin, \\nhad mastered this art.\\n\\nSuddenly she turned from the window and stood before the glass. \\nHer eyes were shining brightly, but her face had lost its color. Quickly \\nshe pulled down her hair and let it fall to its complete length.\\n\\nThe James Dillingham Youngs were very proud of two things which \\nthey owned. One thing was Jim’s gold watch. It had once belonged to  \\nhis father. And, long ago, it had belonged to his father’s father. The  \\nother thing was Della’s hair.\\n\\nIf a queen had lived in the rooms near theirs, Della would have \\nwashed and dried her hair where the queen could see it. Della knew  \\nher hair was more beautiful than any queen’s jewels and gifts.\\n\\nIf a king had lived in the same house, with all his riches, Jim would \\nhave looked at his watch every time they met. Jim knew that no king  \\n\\n2\\n\\n\\x0cT h e   G i\\n\\nf\\n\\nt\\n\\n  o f\\n\\n \\n\\nt h e   M a g i\\n\\nhad anything so valuable.\\n\\nSo now Della’s beautiful hair fell about her, shining like a falling \\nstream of brown water. It reached below her knee. It almost made itself \\ninto a dress for her.\\n\\nAnd then she put it up on her head again, nervously and quickly. \\nOnce she stopped for a moment and stood still while a tear or two ran \\ndown her face.\\n\\nShe put on her old brown coat. She put on her old brown hat.  \\nWith the bright light still in her eyes, she moved quickly out the door \\nand down to the street.\\n\\nWhere she stopped, the sign said: “Mrs. Sofronie. Hair Articles  \\n\\nof all Kinds.”\\n\\nUp to the second floor Della ran, and stopped to get her breath.\\nMrs. Sofronie, large, too white, cold-eyed, looked at her.\\n“Will you buy my hair?” asked Della.\\n“I buy hair,” said Mrs. Sofronie. “Take your hat off and let me look \\n\\nDown fell the brown waterfall.\\n“Twenty  dollars,”  said  Mrs.  Sofronie,  lifting  the  hair  to  feel  its \\n\\nat it.”\\n\\nweight.\\n\\n“Give it to me quick,” said Della.\\nOh, and the next two hours seemed to fly. She was going from  \\n\\none shop to another, to find a gift for Jim.\\n\\nShe found it at last. It surely had been made for Jim and no one \\nelse. There was no other like it in any of the shops, and she had looked \\nin every shop in the city.\\n\\nIt was a gold watch chain, very simply made. Its value was in its  \\nrich and pure material. Because it was so plain and simple, you knew  \\nthat it was very valuable. All good things are like this.\\n\\nIt was good enough for The Watch.\\nAs soon as she saw it, she knew that Jim must have it. It was like \\nhim. Quietness and value—Jim and the chain both had quietness and \\nvalue. She paid twenty-one dollars for it. And she hurried home with  \\nthe chain and eighty-seven cents. \\n\\n3\\n\\n\\x0cO .\\n\\n  H e n r y\\n\\nWith  that  chain  on  his  watch,  Jim  could  look  at  his  watch  and \\nlearn  the  time  anywhere  he  might  be.  Though  the  watch  was  so  fine,  \\nit had never had a fine chain. He sometimes took it out and looked at  \\nit only when no one could see him do it.\\n\\nWhen Della arrived home, her mind quieted a little. She began to  \\nthink more reasonably. She started to try to cover the sad marks of what \\nshe had done. Love and large-hearted giving, when added together, can \\nleave  deep  marks.  It  is  never  easy  to  cover  these  marks,  dear  friends—\\nnever easy.\\n\\nWithin  forty  minutes  her  head  looked  a  little  better.  With  her  \\nshort  hair,  she  looked  wonderfully  like  a  schoolboy.  She  stood  at  the \\nlooking-glass for a long time.\\n\\n“If Jim doesn’t kill me,” she said to herself, “before he looks at me  \\na second time, he’ll say I look like a girl who sings and dances for money. \\nBut what could I do—oh! What could I do with a dollar and eighty-\\nseven cents?”\\n\\nAt seven, Jim’s dinner was ready for him.\\nJim  was  never  late.  Della  held  the  watch  chain  in  her  hand  and \\nsat  near  the  door  where  he  always  entered.  Then  she  heard  his  step  in  \\nthe hall and her face lost color for a moment. She often said little prayers \\nquietly,  about  simple  everyday  things.  And  now  she  said:  “Please  God, \\nmake him think I’m still pretty.”\\n\\nThe door opened and Jim stepped in. He looked very thin and he \\nwas not smiling. Poor fellow, he was only twenty-two—and with a fam-\\nily to take care of! He needed a new coat and he had nothing to cover \\nhis cold hands.\\n\\nJim stopped inside the door. He was as quiet as a hunting dog when \\nit  is  near  a  bird.  His  eyes  looked  strangely  at  Della,  and  there  was  an \\nexpression in them that she could not understand. It filled her with fear. \\nIt was not anger, nor surprise, nor anything she had been ready for. He \\nsimply looked at her with that strange expression on his face.\\n\\nDella went to him.\\n“Jim, dear,” she cried, “don’t look at me like that. I had my hair cut \\noff  and  sold  it.  I  couldn’t  live  through  Christmas  without  giving  you  a  \\n\\n4\\n\\n\\x0cT h e   G i\\n\\nf\\n\\nt\\n\\n  o f\\n\\n \\n\\nt h e   M a g i\\n\\ngift.  My  hair  will  grow  again.  You  won’t  care,  will  you?  My  hair  grows \\nvery  fast.  It’s  Christmas,  Jim.  Let’s  be  happy.  You  don’t  know  what  a \\nnice—what a beautiful nice gift I got for you.”\\n\\n“You’ve  cut  off  your  hair?”  asked  Jim  slowly.  He  seemed  to  labor \\nto  understand  what  had  happened.  He  seemed  not  to  feel  sure  he  \\nknew.\\n\\n“Cut  it  off  and  sold  it,”  said  Della.  “Don’t  you  like  me  now?  I’m  \\n\\nme, Jim. I’m the same without my hair.”\\nJim looked around the room.\\n“You say your hair is gone?” he said.\\n“You  don’t  have  to  look  for  it,”  said  Della.  “It’s  sold,  I  tell  you—\\nsold and gone, too. It’s the night before Christmas, boy. Be good to me, \\nbecause I sold it for you. Maybe the hairs of my head could be counted,” \\nshe  said,  “but  no  one  could  ever  count  my  love  for  you.  Shall  we  eat  \\ndinner, Jim?”\\n\\nJim  put  his  arms  around  his  Della.  For  ten  seconds  let  us  look  in \\nanother  direction.  Eight  dollars  a  week  or  a  million  dollars  a  year— \\nhow  different  are  they?  Someone  may  give  you  an  answer,  but  it  will  \\nbe  wrong.  The  magi  brought  valuable  gifts,  but  that  was  not  among \\nthem. My meaning will be explained soon.\\n\\nFrom inside the coat, Jim took something tied in paper. He threw \\n\\nit upon the table.\\n\\n“I  want  you  to  understand  me,  Dell,”  he  said.  “Nothing  like  a  \\nhaircut  could  make  me  love  you  any  less.  But  if  you’ll  open  that,  you  \\nmay know what I felt when I came in.”\\n\\nWhite  fingers  pulled  off  the  paper.  And  then  a  cry  of  joy;  and  \\n\\nthen a change to tears.\\n\\nFor  there  lay  The  Combs—the  combs  that  Della  had  seen  in  a \\nshop  window  and  loved  for  a  long  time.  Beautiful  combs,  with  jewels, \\nperfect  for  her  beautiful  hair.  She  had  known  they  cost  too  much  for  \\nher  to  buy  them.  She  had  looked  at  them  without  the  least  hope  of  \\nowning them. And now they were hers, but her hair was gone.\\n\\nBut  she  held  them  to  her  heart,  and  at  last  was  able  to  look  up  \\n\\nand say: “My hair grows so fast, Jim!”\\n\\n5\\n\\n\\x0cO .\\n\\n  H e n r y\\n\\nAnd then she jumped up and cried, “Oh, oh!”\\nJim had not yet seen his beautiful gift. She held it out to him in  \\nher open hand. The gold seemed to shine softly as if with her own warm  \\nand loving spirit.\\n\\n“Isn’t it perfect, Jim? I hunted all over town to find it. You’ll have \\nto look at your watch a hundred times a day now. Give me your watch. \\nI want to see how they look together.”\\n\\nJim sat down and smiled.\\n“Della,” said he, “let’s put our Christmas gifts away and keep them  \\na while. They’re too nice to use now. I sold the watch to get the money \\nto buy the combs. And now I think we should have our dinner.”\\n\\nThe magi, as you know, were wise men—wonderfully wise men— \\nwho brought gifts to the newborn Christ-child. They were the first to \\ngive Christmas gifts. Being wise, their gifts were doubtless wise ones. \\nAnd here I have told you the story of two children who were not wise. \\nEach sold the most valuable thing he owned in order to buy a gift for \\nthe other. But let me speak a last word to the wise of these days: Of all  \\nwho give gifts, these two were the most wise. Of all who give and receive  \\ngifts, such as they are the most wise. Everywhere they are the wise ones.  \\nThey are the magi.\\n\\n6\\n\\n\\x0c'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the file from here: http://clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-data-collection-extraction-primer/GiftOfTheMagi.pdf\n",
    "extract_text(\"GiftOfTheMagi.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will only need to use this function for Project 3. For reference, we also provide another useful text extraction function that does so page by page, which you can consider for future projects if, for example, a given pdf file is too large to be parsed all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** PAGE 1 ****\n",
      "\n",
      "The Gift of the MagipThe Gift of the MagiONE DOLLAR AND EIGHTY-SEVEN CENTS.     That was all. She had put it aside, one cent and then another and then another, in her careful buying of meat and other food. Della counted  it three times. One dollar and eighty-seven cents. And the next day  would be Christmas.There was nothing to do but fall on the bed and cry. So Della did it. While the lady of the home is slowly growing quieter, we can  look at the home. Furnished rooms at a cost of $8 a week. There is lit-tle more to say about it.In the hall below was a letter-box too small to hold a letter. There was an electric bell, but it could not make a sound. Also there was a  name beside the door: “Mr. James Dillingham Young.”1\f",
      "\n",
      "\n",
      "**** PAGE 2 ****\n",
      "\n",
      "O. HenryWhen the name was placed there, Mr. James Dillingham Young was being paid $30 a week. Now, when he was being paid only $20 a week, the name seemed too long and important. It should perhaps have been “Mr. James D. Young.” But when Mr. James Dillingham Young entered the furnished rooms, his name became very short indeed. Mrs. James Dillingham Young put her arms warmly about him and called  him “Jim.” You have already met her. She is Della.Della finished her crying and cleaned the marks of it from her face. She stood by the window and looked out with no interest. Tomorrow would be Christmas Day, and she had only $1.87 with which to buy  Jim a gift. She had put aside as much as she could for months, with this result. Twenty dollars a week is not much. Everything had cost more than she had expected. It always happened like that.Only $ 1.87 to buy a gift for Jim. Her Jim. She had had many happy hours planning something nice for him. Something nearly good enough. Something almost worth the honor of belonging to Jim.There was a looking-glass between the windows of the room. Per-haps you have seen the kind of looking-glass that is placed in $8 fur-nished rooms. It was very narrow. A person could see only a little of himself at a time. However, if he was very thin and moved very quickly, he might be able to get a good view of himself. Della, being quite thin, had mastered this art.Suddenly she turned from the window and stood before the glass. Her eyes were shining brightly, but her face had lost its color. Quickly she pulled down her hair and let it fall to its complete length.The James Dillingham Youngs were very proud of two things which they owned. One thing was Jim’s gold watch. It had once belonged to  his father. And, long ago, it had belonged to his father’s father. The  other thing was Della’s hair.If a queen had lived in the rooms near theirs, Della would have washed and dried her hair where the queen could see it. Della knew  her hair was more beautiful than any queen’s jewels and gifts.If a king had lived in the same house, with all his riches, Jim would have looked at his watch every time they met. Jim knew that no king  2\f",
      "\n",
      "\n",
      "**** PAGE 3 ****\n",
      "\n",
      "The Gift of the Magihad anything so valuable.So now Della’s beautiful hair fell about her, shining like a falling stream of brown water. It reached below her knee. It almost made itself into a dress for her.And then she put it up on her head again, nervously and quickly. Once she stopped for a moment and stood still while a tear or two ran down her face.She put on her old brown coat. She put on her old brown hat.  With the bright light still in her eyes, she moved quickly out the door and down to the street.Where she stopped, the sign said: “Mrs. Sofronie. Hair Articles  of all Kinds.”Up to the second floor Della ran, and stopped to get her breath.Mrs. Sofronie, large, too white, cold-eyed, looked at her.“Will you buy my hair?” asked Della.“I buy hair,” said Mrs. Sofronie. “Take your hat off and let me look at it.”Down fell the brown waterfall.“Twenty dollars,” said Mrs. Sofronie, lifting the hair to feel its weight.“Give it to me quick,” said Della.Oh, and the next two hours seemed to fly. She was going from  one shop to another, to find a gift for Jim.She found it at last. It surely had been made for Jim and no one else. There was no other like it in any of the shops, and she had looked in every shop in the city.It was a gold watch chain, very simply made. Its value was in its  rich and pure material. Because it was so plain and simple, you knew  that it was very valuable. All good things are like this.It was good enough for The Watch.As soon as she saw it, she knew that Jim must have it. It was like him. Quietness and value—Jim and the chain both had quietness and value. She paid twenty-one dollars for it. And she hurried home with  the chain and eighty-seven cents. 3\f",
      "\n",
      "\n",
      "**** PAGE 4 ****\n",
      "\n",
      "O. HenryWith that chain on his watch, Jim could look at his watch and learn the time anywhere he might be. Though the watch was so fine,  it had never had a fine chain. He sometimes took it out and looked at  it only when no one could see him do it.When Della arrived home, her mind quieted a little. She began to  think more reasonably. She started to try to cover the sad marks of what she had done. Love and large-hearted giving, when added together, can leave deep marks. It is never easy to cover these marks, dear friends—never easy.Within forty minutes her head looked a little better. With her  short hair, she looked wonderfully like a schoolboy. She stood at the looking-glass for a long time.“If Jim doesn’t kill me,” she said to herself, “before he looks at me  a second time, he’ll say I look like a girl who sings and dances for money. But what could I do—oh! What could I do with a dollar and eighty-seven cents?”At seven, Jim’s dinner was ready for him.Jim was never late. Della held the watch chain in her hand and sat near the door where he always entered. Then she heard his step in  the hall and her face lost color for a moment. She often said little prayers quietly, about simple everyday things. And now she said: “Please God, make him think I’m still pretty.”The door opened and Jim stepped in. He looked very thin and he was not smiling. Poor fellow, he was only twenty-two—and with a fam-ily to take care of! He needed a new coat and he had nothing to cover his cold hands.Jim stopped inside the door. He was as quiet as a hunting dog when it is near a bird. His eyes looked strangely at Della, and there was an expression in them that she could not understand. It filled her with fear. It was not anger, nor surprise, nor anything she had been ready for. He simply looked at her with that strange expression on his face.Della went to him.“Jim, dear,” she cried, “don’t look at me like that. I had my hair cut off and sold it. I couldn’t live through Christmas without giving you a  4\f",
      "\n",
      "\n",
      "**** PAGE 5 ****\n",
      "\n",
      "The Gift of the Magigift. My hair will grow again. You won’t care, will you? My hair grows very fast. It’s Christmas, Jim. Let’s be happy. You don’t know what a nice—what a beautiful nice gift I got for you.”“You’ve cut off your hair?” asked Jim slowly. He seemed to labor to understand what had happened. He seemed not to feel sure he  knew.“Cut it off and sold it,” said Della. “Don’t you like me now? I’m  me, Jim. I’m the same without my hair.”Jim looked around the room.“You say your hair is gone?” he said.“You don’t have to look for it,” said Della. “It’s sold, I tell you—sold and gone, too. It’s the night before Christmas, boy. Be good to me, because I sold it for you. Maybe the hairs of my head could be counted,” she said, “but no one could ever count my love for you. Shall we eat  dinner, Jim?”Jim put his arms around his Della. For ten seconds let us look in another direction. Eight dollars a week or a million dollars a year— how different are they? Someone may give you an answer, but it will  be wrong. The magi brought valuable gifts, but that was not among them. My meaning will be explained soon.From inside the coat, Jim took something tied in paper. He threw it upon the table.“I want you to understand me, Dell,” he said. “Nothing like a  haircut could make me love you any less. But if you’ll open that, you  may know what I felt when I came in.”White fingers pulled off the paper. And then a cry of joy; and  then a change to tears.For there lay The Combs—the combs that Della had seen in a shop window and loved for a long time. Beautiful combs, with jewels, perfect for her beautiful hair. She had known they cost too much for  her to buy them. She had looked at them without the least hope of  owning them. And now they were hers, but her hair was gone.But she held them to her heart, and at last was able to look up  and say: “My hair grows so fast, Jim!”5\f",
      "\n",
      "\n",
      "**** PAGE 6 ****\n",
      "\n",
      "O. HenryAnd then she jumped up and cried, “Oh, oh!”Jim had not yet seen his beautiful gift. She held it out to him in  her open hand. The gold seemed to shine softly as if with her own warm  and loving spirit.“Isn’t it perfect, Jim? I hunted all over town to find it. You’ll have to look at your watch a hundred times a day now. Give me your watch. I want to see how they look together.”Jim sat down and smiled.“Della,” said he, “let’s put our Christmas gifts away and keep them  a while. They’re too nice to use now. I sold the watch to get the money to buy the combs. And now I think we should have our dinner.”The magi, as you know, were wise men—wonderfully wise men— who brought gifts to the newborn Christ-child. They were the first to give Christmas gifts. Being wise, their gifts were doubtless wise ones. And here I have told you the story of two children who were not wise. Each sold the most valuable thing he owned in order to buy a gift for the other. But let me speak a last word to the wise of these days: Of all  who give gifts, these two were the most wise. Of all who give and receive  gifts, such as they are the most wise. Everywhere they are the wise ones.  They are the magi.6\f",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import io\n",
    "\n",
    "# source: https://www.blog.pythonlibrary.org/2018/05/03/exporting-data-from-pdfs-with-python/\n",
    "def extract_text_by_page(pdf_path):\n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        for page in PDFPage.get_pages(fh, caching = True, check_extractable = True):\n",
    "            resource_manager = PDFResourceManager()\n",
    "            fake_file_handle = io.StringIO()\n",
    "            converter = TextConverter(resource_manager, fake_file_handle)\n",
    "            page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "            page_interpreter.process_page(page)\n",
    "\n",
    "            text = fake_file_handle.getvalue()\n",
    "            yield text\n",
    "\n",
    "            converter.close()\n",
    "            fake_file_handle.close()\n",
    "            \n",
    "for index, page in enumerate(extract_text_by_page(\"GiftOfTheMagi.pdf\")):\n",
    "    print(f\"**** PAGE {index + 1} ****\\n\")\n",
    "    print(page)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pdfminer` also offers other functionalities, such as extracting images from pdf files, which you can read more in the [official documentation](https://pdfminersix.readthedocs.io/en/latest/index.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
